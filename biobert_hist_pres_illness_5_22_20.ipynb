{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "biobert_hist_pres_illness_5/22/20.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elliehaber/Iris_Classification/blob/master/biobert_hist_pres_illness_5_22_20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4Nue0XaDNNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnMFarmf0PKV",
        "colab_type": "code",
        "outputId": "77a770e1-2836-4907-b244-7c39df5e1e4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4c4KqOgDYIu",
        "colab_type": "code",
        "outputId": "d10cfb2d-f563-4afc-f91d-72d70e6d2b81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-hMPT8ODYLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BTb-KRlDYN8",
        "colab_type": "code",
        "outputId": "b6655c6d-9b71-492f-92aa-4612c2e417fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#patient notes data\n",
        "df_notes = pd.read_csv('NOTEEVENTS.csv')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1eKuRisGShT",
        "colab_type": "code",
        "outputId": "f1e3fc17-2608-4b6b-c11f-22eac496da24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(df_notes)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2083180"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldqGAT4_DYRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#icd diagnoses data\n",
        "df_icd_diag = pd.read_csv('DIAGNOSES_ICD 2.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTMZUMjJMtEE",
        "colab_type": "code",
        "outputId": "d60206cf-07e9-4b94-c4c5-eb31bbba210f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "#Identify the top 10 occurring ICD-9 Codes\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_icd_diag.head()\n",
        "freq = df_icd_diag['ICD9_CODE'].value_counts()[:10].index.tolist()\n",
        "x = df_icd_diag['ICD9_CODE'].value_counts()[:50].index.tolist()\n",
        "\n",
        "\n",
        "print(sum(df_icd_diag['ICD9_CODE'].value_counts()[:10]))\n",
        "freq_val = df_icd_diag['ICD9_CODE'].value_counts()[:10]\n",
        "plt.bar(freq, freq_val)\n",
        "plt.title('Top Ten ICD-9 Codes')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('ICD9-Codes')\n",
        "\n",
        "print(x)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "106379\n",
            "['4019', '4280', '42731', '41401', '5849', '25000', '2724', '51881', '5990', '53081', '2720', 'V053', 'V290', '2859', '2449', '486', '2851', '2762', '496', '99592', 'V5861', '0389', '5070', 'V3000', '5859', '311', '40390', '3051', '412', '2875', 'V4581', '41071', '2761', '4240', 'V1582', 'V3001', '5119', 'V4582', '40391', '78552', '4241', 'V5867', '42789', '32723', '9971', '5845', '2760', '7742', '49390', '2767']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dRLbyi1Phse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Filter for notes with discharge summary\n",
        "df_notes_discharge_summ = df_notes.loc[df_notes.CATEGORY == 'Discharge summary']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxNH5cx5W1Cq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_notes_discharge_summ_last = (df_notes_discharge_summ.groupby(['SUBJECT_ID', 'HADM_ID']).nth(-1)).reset_index()\n",
        "assert df_notes_discharge_summ_last.duplicated(['HADM_ID']).sum() == 0, 'Multiple discharge summaries per admission'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voyXheqEQW2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_icd_notes = pd.merge(df_icd_diag[['SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE']], \\\n",
        "df_notes_discharge_summ_last[['SUBJECT_ID', 'HADM_ID', 'TEXT']], on= ['SUBJECT_ID', 'HADM_ID'], how = 'left'\n",
        ")\n",
        "\n",
        "assert len(df_icd_diag) == len(df_icd_notes), 'Number of rows increased'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzHycoXwxexz",
        "colab_type": "code",
        "outputId": "e9579a85-233b-414e-b503-69b05d19f459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#filter for first admission\n",
        "df_icd_notes_1 = df_icd_notes[df_icd_notes.ICD9_CODE.isin(x)]\n",
        "print(len(df_icd_notes_1))\n",
        "df_icd_notes = df_icd_notes[df_icd_notes.ICD9_CODE.isin(freq)]\n",
        "df_icd_notes_1 = df_icd_notes[df_icd_notes.ICD9_CODE.isin(x)]\n",
        "\n",
        "t = df_icd_notes.drop_duplicates(subset='HADM_ID', keep=\"first\", inplace=True)\n",
        "df_icd_notes = df_icd_notes.dropna(how='any',subset=['TEXT'])\n",
        "\n",
        "len(df_icd_notes)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "242728\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40562"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD0MO5VMDJ8S",
        "colab_type": "code",
        "outputId": "6f379c19-0b7d-4bb1-fe46-071a5ca51cd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "x = df_icd_notes.TEXT.values[:1]\n",
        "for i in x:\n",
        "  print(i)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Admission Date: [**2194-6-13**]        Discharge Date: [**2194-6-18**]\n",
            "\n",
            "\n",
            "Service:\n",
            "\n",
            "\n",
            "HISTORY OF PRESENT ILLNESS:  The patient is a [**Age over 90 **]-year-old man\n",
            "with a history of peptic ulcer disease, coronary artery\n",
            "disease, status post myocardial infarction in [**2179**] as well as\n",
            "[**2193**], temporal arteritis, who presented with melenas  and\n",
            "chest pain. The patient reported melanotic stools times 5\n",
            "since 4 p.m. on the day prior to admission. No hematemesis or\n",
            "hematochezia. Stools were loose. The patient had a history of\n",
            "melena in [**2192-5-14**]. The patient also reported being\n",
            "lightheaded, fatigued with an increase in his ch set\n",
            "discomfort for which he was taking sublingual nitroglycerin\n",
            "with relief. On the a.m. of presentation, the symptoms\n",
            "persisted; the patient contact[**Name (NI) **] his PCP who sent him to the\n",
            "[**Name (NI) **]. In the ED, the p was found to have a hematocrit of 22.9\n",
            "decreased from a baseline of 32 to 38. He was given IV\n",
            "Protonix, IV fluids, and transfused the first of 2 units of\n",
            "packed red blood cells. Gastroenterology was consulted. The\n",
            "patient initially had an EKG with slight inferior changes\n",
            "while the patient was pain free. The patient then had an\n",
            "episode of [**9-23**] substernal chest pain in the ED with 3 to [**Street Address(2) 94587**] changes in V3 to V4.\n",
            "\n",
            "PAST MEDICAL HISTORY:  Significant for upper gastrointestinal\n",
            "bleed in [**2192-5-14**]. An esophagogastroduodenoscopy showed an\n",
            "ulcer in the pylorus and chronic gastritis, coronary artery\n",
            "disease, status post myocardial infarction in [**2179**] and [**2193**],\n",
            "benign prostatic hypertrophy, history of temporal arteritis,\n",
            "pemphigoid, history of anemia, history of small bowel\n",
            "volvulus, status post appendectomies, status post inguinal\n",
            "hernia repair x2, history of colonic polyps, and sigmoid\n",
            "diverticulosis.\n",
            "\n",
            "ALLERGIES:  THE PATIENT HAS NO KNOWN DRUG ALLERGIES.\n",
            "\n",
            "MEDICATIONS:  The patient was on:\n",
            "\n",
            "1. Celebrex.\n",
            "2. Aspirin.\n",
            "3. Prednisone.\n",
            "4. Atenolol.\n",
            "5. Imdur.\n",
            "6. Nitroglycerin p.r.n.\n",
            "\n",
            "\n",
            "SOCIAL HISTORY:  He is a retired physician, [**Name Initial (NameIs) 2447**].\n",
            "Remote tobacco history. Social alcohol use, which is\n",
            "infrequent. Married with 1 son.\n",
            "\n",
            "FAMILY HISTORY:  Noncontributory.\n",
            "\n",
            "PHYSICAL EXAMINATION ON ADMISSION AS FOLLOWS:  VITAL SIGNS:\n",
            "Vital signs of 98.9 temperature, blood pressure 128/80, pulse\n",
            "72, respiratory rate of 13, and oxygen 100% on 3 liters.\n",
            "GENERAL: The patient appeared comfortable.\n",
            "HEENT: Examination was unremarkable except for pale\n",
            "conjunctiva, dry mucosa.\n",
            "\n",
            "LABORATORY DATA:  Significant for the hematocrit of 23 as\n",
            "stated above, a potassium of 5.3, a BUN 78. Initial CK was\n",
            "107 with an MB of 6 and a troponin of 0.02. INR was 1.0.\n",
            "Urinalysis was unremarkable. As stated above, the patient had\n",
            "2 EKGs and the second of which showed 2 to [**Street Address(2) 5366**] changes in\n",
            "V3 to V6. Chest x-ray showed no acute cardiopulmonary\n",
            "process, so the patient was admitted to the hospital.\n",
            "\n",
            "CONCISE SUMMARY OF HOSPITAL COURSE AS FOLLOWS:  GI: The\n",
            "patient was felt to likely have another bleeding ulcer as the\n",
            "etiology of his melanotic stools and anemia. The patient had\n",
            "a history of Helicobacter pylori in the past that was\n",
            "treated. The patient was felt to require EGD to evaluate for\n",
            "recurrent infection as well as ongoing bleeding. The patient\n",
            "was initially admitted to the ICU. Gastroenterology was\n",
            "consulted. The patient was taken for EGD on [**6-13**], which\n",
            "showed a deep antral ulcer, no acute bleeding. The ulcer was\n",
            "injected.\n",
            "\n",
            "The patient was initially continued on IV b.i.d. Protonix.\n",
            "Hematocrits were followed and the patient was maintained on 2\n",
            "peripheral IV's at all times, and aspirin was held. The\n",
            "patient has another episode of melanotic stool. On [**2194-6-14**],\n",
            "he was taken for another EGD, at that time which showed the\n",
            "ulcer was not bleeding. As a result, the patient was felt to\n",
            "be stable for discharge to home from a GI perspective with\n",
            "continuation of the b.i.d. Protonix. The patient to follow up\n",
            "for a repeat endoscopy in 8 weeks as an outpatient.\n",
            "\n",
            "Cardiac: Cardiac enzymes had been significant for elevated\n",
            "troponin on admission. Cardiology was contact[**Name (NI) **] who did not\n",
            "recommend cardiac catheterization or coronary artery bypass\n",
            "graft. The patient initially received heparin and was\n",
            "restarted on aspirin, which was approved by GI as long as the\n",
            "patient had serial hematocrits. The patient was transfused to\n",
            "keep the hematocrit above 30. He was restarted on atenolol.\n",
            "The patient was also on Imdur for a longer-acting vasodilator\n",
            "effect. The patient had a couple of episodes of further chest\n",
            "pain during the admission but had no further EKG changes.\n",
            "\n",
            "Pulmonary: The patient had some desaturations to 70's and\n",
            "80's with ambulation without improvement with oxygen with\n",
            "ambulation, but at this time the patient was completely\n",
            "asymptomatic and the patient's oxygen saturation recovered\n",
            "spontaneously to the high 90's on room air with rest. As a\n",
            "result, this was felt to possibly be not reflective of the\n",
            "patient's pulmonary status, but reflective of some peripheral\n",
            "vascular changes with ambulation. The patient was not felt to\n",
            "need inpatient workup and will follow up with PCP as an\n",
            "outpatient.\n",
            "\n",
            "Hematology: The patient with acute blood loss anemia,\n",
            "received a total of 4 units of packed red blood cells, had\n",
            "serial hematocrits while on heparin gtt and was transfused to\n",
            "keep the hematocrit above 30.\n",
            "\n",
            "Musculoskeletal: The patient was restarted on his prednisone\n",
            "for polymyalgia rheumatica and temporal arteritis.\n",
            "\n",
            "DISCHARGE DIAGNOSES:  Gastric ulcer.\n",
            "Gastrointestinal bleed.\n",
            "Demand ischemia, elevated troponins, and EKG changes in the\n",
            "setting of acute blood loss anemia.\n",
            "\n",
            "DISCHARGE MEDICATIONS:\n",
            "1. Nitroglycerin sublingual.\n",
            "2. Prednisone 5 mg p.o. daily.\n",
            "3. Atenolol 25 mg p.o. q. p.m., 50 mg p.o. q. a.m.\n",
            "4. Protonix 40 p.o. b.i.d.\n",
            "5. Aspirin 325 mg.\n",
            "6. Isosorbide mononitrate.\n",
            "\n",
            "\n",
            "DISCHARGE FOLLOWUP:  Follow up with Cardiology on [**Last Name (LF) 2974**], [**6-20**], at 9:15 a.m. The patient's primary cardiologist is Dr.\n",
            "[**Last Name (STitle) 104122**]. Dr. [**Last Name (STitle) 104122**] was not available so the patient\n",
            "followed up with Dr. [**Last Name (STitle) 11378**]. The patient also followed up with\n",
            "Dr. [**Last Name (STitle) **] for outpatient endoscopy on [**8-21**] at 12:30 p.m.\n",
            "and the patient was suggested to pursue cardiac\n",
            "rehabilitation in 4 to 6 weeks.\n",
            "\n",
            "\n",
            "\n",
            "                        [**First Name11 (Name Pattern1) **] [**Last Name (NamePattern1) **], [**MD Number(1) 5825**]\n",
            "\n",
            "Dictated By:[**Last Name (NamePattern1) 8160**]\n",
            "MEDQUIST36\n",
            "D:  [**2194-8-1**] 12:24:02\n",
            "T:  [**2195-5-15**] 13:09:19\n",
            "Job#:  [**Job Number 94529**]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ9wxKM1X3fa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extract Hospital Course section of notes\n",
        "\n",
        "df_icd_notes\n",
        "lst = []\n",
        "for i in df_icd_notes['TEXT']:\n",
        "  i = str(i).lower()\n",
        "  if 'concise summary of hospital course as follows:' in i:\n",
        "    start = i.find('concise summary of hospital course as follows:') \n",
        "    lst.append(i[start+46:])\n",
        "  elif 'hospital course:' in i:\n",
        "    start = i.find('hospital course:') \n",
        "    lst.append(i[start+16:])\n",
        "  else: \n",
        "    lst.append('not valid')\n",
        "\n",
        "df_icd_notes['Hosp_course'] = lst\n",
        "\n",
        "df_icd_notes = df_icd_notes[df_icd_notes.Hosp_course != 'not valid']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BHsClkxZVz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extract History of Present Illness section of notes\n",
        "\n",
        "df_icd_notes\n",
        "lst = []\n",
        "for i in df_icd_notes['TEXT']:\n",
        "  i = str(i).lower()\n",
        "  if 'history of present illness' in i:\n",
        "    start = i.find('history of present illness:')\n",
        "    lst.append(i[start+27:])\n",
        "  else:\n",
        "    lst.append('not valid')\n",
        "\n",
        "\n",
        "df_icd_notes['hist_pres_illness'] = lst\n",
        "\n",
        "df_icd_notes = df_icd_notes[df_icd_notes.hist_pres_illness != 'not valid']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OJCrsl2-M39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_icd_notes = df_icd_notes.dropna(how='any',subset=['Hosp_course'])\n",
        "df_icd_notes = df_icd_notes.dropna(how='any',subset=['hist_pres_illness'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D8g6d6TNJaV",
        "colab_type": "code",
        "outputId": "d96518bc-2e3a-4072-896b-0a941870d1a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-5eFsnaNMPG",
        "colab_type": "code",
        "outputId": "ad12a099-0dc5-4f56-f101-709b802ed0f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert pytorch-nlp"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.13.13)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.2.0)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.16.13)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.0,>=1.16.13->boto3->pytorch-pretrained-bert) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owrv_lBUa75W",
        "colab_type": "code",
        "outputId": "6da788c7-1ee1-4e04-9df5-33f3c2fc0902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "source": [
        "!pip install biobert-embedding"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: biobert-embedding in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from biobert-embedding) (2.2.0)\n",
            "Requirement already satisfied: torch==1.2.0 in /usr/local/lib/python3.6/dist-packages (from biobert-embedding) (1.2.0)\n",
            "Requirement already satisfied: pytorch-pretrained-bert==0.6.2 in /usr/local/lib/python3.6/dist-packages (from biobert-embedding) (0.6.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->biobert-embedding) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->biobert-embedding) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->biobert-embedding) (1.18.4)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->biobert-embedding) (2.2.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->biobert-embedding) (1.4.1)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->biobert-embedding) (2.2.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->biobert-embedding) (1.29.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->biobert-embedding) (0.34.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->biobert-embedding) (1.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->biobert-embedding) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->biobert-embedding) (3.2.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->biobert-embedding) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->biobert-embedding) (3.10.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->biobert-embedding) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->biobert-embedding) (1.1.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->biobert-embedding) (0.3.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->biobert-embedding) (1.6.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.6.2->biobert-embedding) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.6.2->biobert-embedding) (1.13.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.6.2->biobert-embedding) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.6.2->biobert-embedding) (4.41.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->biobert-embedding) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->biobert-embedding) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->biobert-embedding) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->biobert-embedding) (1.6.0.post3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->biobert-embedding) (46.3.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->biobert-embedding) (1.7.2)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert==0.6.2->biobert-embedding) (1.16.13)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert==0.6.2->biobert-embedding) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert==0.6.2->biobert-embedding) (0.3.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.6.2->biobert-embedding) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.6.2->biobert-embedding) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.6.2->biobert-embedding) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.6.2->biobert-embedding) (2020.4.5.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow->biobert-embedding) (1.6.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->biobert-embedding) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->biobert-embedding) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->biobert-embedding) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->biobert-embedding) (4.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->pytorch-pretrained-bert==0.6.2->biobert-embedding) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->pytorch-pretrained-bert==0.6.2->biobert-embedding) (2.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow->biobert-embedding) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->biobert-embedding) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->biobert-embedding) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjqEETz4NdLP",
        "colab_type": "code",
        "outputId": "b6091e59-9dca-40f2-840a-14d4495bba75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD9wjmq8d5QN",
        "colab_type": "code",
        "outputId": "3d6ae3a6-6a24-4cbf-bbd4-968ac05d6280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "pip install biobert-embedding==0.1.1\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement biobert-embedding==0.1.1 (from versions: 0.1.2)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for biobert-embedding==0.1.1\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk8VjZ9kNTZs",
        "colab_type": "code",
        "outputId": "6f214c94-48c3-4d3f-9922-3fc5978e9038",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGtsVbprNWGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer() \n",
        "import string\n",
        "\n",
        "\n",
        "def preprocess(text):  \n",
        "    punc_list = string.punctuation+'0123456789'\n",
        "    t = str.maketrans(dict.fromkeys(punc_list, \"\"))\n",
        "    text = text.lower().translate(t)\n",
        "    text = text.replace('\\n','')\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XJ3j2vwNYDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# labels_dict = {}\n",
        "# num = 0\n",
        "# for i in freq:\n",
        "#   labels_dict[str(i)] = num\n",
        "#   num+= 1\n",
        "\n",
        "# print(labels_dict)\n",
        "class2idx = {\n",
        "    '4019': 0,\n",
        "    '4280': 1,\n",
        "    '42731': 2,\n",
        "    '41401': 3,\n",
        "    '5849': 4,\n",
        "    '25000': 5,\n",
        "    '2724': 6,\n",
        "    '51881': 7,\n",
        "    '5990': 8,\n",
        "    '53081': 9\n",
        "}\n",
        "\n",
        "# class2idx = {\n",
        "#     4019: 0,\n",
        "#     4280: 1,\n",
        "#     42731: 2,\n",
        "#     41401: 3,\n",
        "#     5849: 4,\n",
        "#     25000: 5,\n",
        "#     2724: 6,\n",
        "#     51881: 7,\n",
        "#     5990: 8,\n",
        "#     53081: 9\n",
        "# }\n",
        "\n",
        "idx2class = {v: k for k, v in class2idx.items()}\n",
        "\n",
        "\n",
        "df_icd_notes['ICD9_CODE'].replace(class2idx, inplace=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "460JbmzZNZ4D",
        "colab_type": "code",
        "outputId": "b0e9c57c-c60c-4f2f-a440-6c02f613e348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        }
      },
      "source": [
        "df_icd_notes"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT_ID</th>\n",
              "      <th>HADM_ID</th>\n",
              "      <th>SEQ_NUM</th>\n",
              "      <th>ICD9_CODE</th>\n",
              "      <th>TEXT</th>\n",
              "      <th>Hosp_course</th>\n",
              "      <th>hist_pres_illness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>112</td>\n",
              "      <td>174105</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3</td>\n",
              "      <td>Admission Date: [**2194-6-13**]        Dischar...</td>\n",
              "      <td>gi: the\\npatient was felt to likely have ano...</td>\n",
              "      <td>the patient is a [**age over 90 **]-year-old...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>113</td>\n",
              "      <td>109976</td>\n",
              "      <td>3.0</td>\n",
              "      <td>9</td>\n",
              "      <td>Admission Date: [**2140-12-12**]        Discha...</td>\n",
              "      <td>the patient was admitted to the intensive\\nc...</td>\n",
              "      <td>the patient is a 35 year old\\ngentleman who ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>114</td>\n",
              "      <td>178393</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "      <td>Admission Date:  [**2146-8-29**]       Dischar...</td>\n",
              "      <td>the patient was admitted to the [**hospital1...</td>\n",
              "      <td>this is a 48-year-old man in\\ngenerally good...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>115</td>\n",
              "      <td>114585</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8</td>\n",
              "      <td>Admission Date:  [**2194-10-16**]             ...</td>\n",
              "      <td>\\nshe was taken to the or by dr. [**first name...</td>\n",
              "      <td>\\nthe patient is a 75 y/o female who presents ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>117</td>\n",
              "      <td>140784</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Admission Date:  [**2133-4-7**]     Discharge ...</td>\n",
              "      <td>\\n1.  hypotension.  the patient was admitted t...</td>\n",
              "      <td>the patient is a 49 year old\\nwoman with a h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>650962</th>\n",
              "      <td>97164</td>\n",
              "      <td>109302</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7</td>\n",
              "      <td>Admission Date:  [**2134-11-26**]             ...</td>\n",
              "      <td>\\nassessment and plan: ms. [**known lastname 2...</td>\n",
              "      <td>\\nms. [**known lastname 2564**] is an 83 y/o f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>650986</th>\n",
              "      <td>97484</td>\n",
              "      <td>172304</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Admission Date:  [**2196-8-19**]              ...</td>\n",
              "      <td>\\nthe patient tolerated her procedure well, an...</td>\n",
              "      <td>\\n79 f with no past oncologic history who pres...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>650993</th>\n",
              "      <td>97488</td>\n",
              "      <td>152542</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2</td>\n",
              "      <td>Admission Date:  [**2128-4-8**]              D...</td>\n",
              "      <td>\\nmr. [**known lastname 17811**] was admitted ...</td>\n",
              "      <td>\\n66m transferred from [**hospital1 18**] [**l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>651007</th>\n",
              "      <td>97488</td>\n",
              "      <td>161999</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8</td>\n",
              "      <td>Admission Date:  [**2128-8-27**]              ...</td>\n",
              "      <td>\\n*)neuro: patient was admitted [**2128-8-27**...</td>\n",
              "      <td>\\nthe patient is a 67 year old right handed ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>651038</th>\n",
              "      <td>97497</td>\n",
              "      <td>168949</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2</td>\n",
              "      <td>Admission Date:  [**2106-7-16**]              ...</td>\n",
              "      <td>\\n# paroxysmal atrial fibrillation:  patient h...</td>\n",
              "      <td>\\ndr [**known lastname 8993**] is a 57 yo man ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>34844 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        SUBJECT_ID  ...                                  hist_pres_illness\n",
              "31             112  ...    the patient is a [**age over 90 **]-year-old...\n",
              "35             113  ...    the patient is a 35 year old\\ngentleman who ...\n",
              "36             114  ...    this is a 48-year-old man in\\ngenerally good...\n",
              "50             115  ...  \\nthe patient is a 75 y/o female who presents ...\n",
              "67             117  ...    the patient is a 49 year old\\nwoman with a h...\n",
              "...            ...  ...                                                ...\n",
              "650962       97164  ...  \\nms. [**known lastname 2564**] is an 83 y/o f...\n",
              "650986       97484  ...  \\n79 f with no past oncologic history who pres...\n",
              "650993       97488  ...  \\n66m transferred from [**hospital1 18**] [**l...\n",
              "651007       97488  ...  \\nthe patient is a 67 year old right handed ma...\n",
              "651038       97497  ...  \\ndr [**known lastname 8993**] is a 57 yo man ...\n",
              "\n",
              "[34844 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIrujGLONbnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = df_icd_notes.ICD9_CODE.values\n",
        "labels = labels.astype(np.long)\n",
        "\n",
        "sentences_hosp = df_icd_notes.hist_pres_illness.values\n",
        "sentences_hosp = sentences_hosp.astype('str')\n",
        "\n",
        "test_labels = labels[27001:28500]\n",
        "test_sent = sentences_hosp[27001:28500]\n",
        "\n",
        "# test_labels = labels[2700:3020]\n",
        "# test_sent = sentences_hosp[2700:3020]\n",
        "\n",
        "for i in range(len(test_sent)):\n",
        "    sent = test_sent[i]\n",
        "    sent = preprocess(sent)\n",
        "    test_sent[i] = sent\n",
        "# sentences_hist = df_icd_notes.hist_pres_illness.values\n",
        "# sentences_hist = sentences_hist.astype(np.string_)\n",
        "# sentences = df_icd_notes.TEXT.values\n",
        "# sentences = sentences.astype(np.string_)\n",
        "\n",
        "\n",
        "# labels = labels[:27000]\n",
        "# sentences_hosp = sentences_hosp[:27000]\n",
        "labels = labels[:15000]\n",
        "sentences_hosp = sentences_hosp[:15000]\n",
        "\n",
        "\n",
        "for i in range(len(sentences_hosp)):\n",
        "    sent = sentences_hosp[i]\n",
        "    sent = preprocess(sent)\n",
        "    sentences_hosp[i] = sent\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJahOv9LXwmM",
        "colab_type": "code",
        "outputId": "62b18440-c454-4685-8ffd-4bb7e61b8ff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sentences_hosp[0]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  the patient is a age over  yearold manwith a history of peptic ulcer disease coronary arterydisease status post myocardial infarction in  as well as temporal arteritis who presented with melenas  andchest pain the patient reported melanotic stools times since  pm on the day prior to admission no hematemesis orhematochezia stools were loose the patient had a history ofmelena in  the patient also reported beinglightheaded fatigued with an increase in his ch setdiscomfort for which he was taking sublingual nitroglycerinwith relief on the am of presentation the symptomspersisted the patient contactname ni  his pcp who sent him to thename ni  in the ed the p was found to have a hematocrit of decreased from a baseline of  to  he was given ivprotonix iv fluids and transfused the first of  units ofpacked red blood cells gastroenterology was consulted thepatient initially had an ekg with slight inferior changeswhile the patient was pain free the patient then had anepisode of  substernal chest pain in the ed with  to street address  changes in v to vpast medical history  significant for upper gastrointestinalbleed in  an esophagogastroduodenoscopy showed anulcer in the pylorus and chronic gastritis coronary arterydisease status post myocardial infarction in  and benign prostatic hypertrophy history of temporal arteritispemphigoid history of anemia history of small bowelvolvulus status post appendectomies status post inguinalhernia repair x history of colonic polyps and sigmoiddiverticulosisallergies  the patient has no known drug allergiesmedications  the patient was on celebrex aspirin prednisone atenolol imdur nitroglycerin prnsocial history  he is a retired physician name initial nameis remote tobacco history social alcohol use which isinfrequent married with  sonfamily history  noncontributoryphysical examination on admission as follows  vital signsvital signs of  temperature blood pressure  pulse respiratory rate of  and oxygen  on  litersgeneral the patient appeared comfortableheent examination was unremarkable except for paleconjunctiva dry mucosalaboratory data  significant for the hematocrit of  asstated above a potassium of  a bun  initial ck was with an mb of  and a troponin of  inr was urinalysis was unremarkable as stated above the patient had ekgs and the second of which showed  to street address  changes inv to v chest xray showed no acute cardiopulmonaryprocess so the patient was admitted to the hospitalconcise summary of hospital course as follows  gi thepatient was felt to likely have another bleeding ulcer as theetiology of his melanotic stools and anemia the patient hada history of helicobacter pylori in the past that wastreated the patient was felt to require egd to evaluate forrecurrent infection as well as ongoing bleeding the patientwas initially admitted to the icu gastroenterology wasconsulted the patient was taken for egd on  whichshowed a deep antral ulcer no acute bleeding the ulcer wasinjectedthe patient was initially continued on iv bid protonixhematocrits were followed and the patient was maintained on peripheral ivs at all times and aspirin was held thepatient has another episode of melanotic stool on he was taken for another egd at that time which showed theulcer was not bleeding as a result the patient was felt tobe stable for discharge to home from a gi perspective withcontinuation of the bid protonix the patient to follow upfor a repeat endoscopy in  weeks as an outpatientcardiac cardiac enzymes had been significant for elevatedtroponin on admission cardiology was contactname ni  who did notrecommend cardiac catheterization or coronary artery bypassgraft the patient initially received heparin and wasrestarted on aspirin which was approved by gi as long as thepatient had serial hematocrits the patient was transfused tokeep the hematocrit above  he was restarted on atenololthe patient was also on imdur for a longeracting vasodilatoreffect the patient had a couple of episodes of further chestpain during the admission but had no further ekg changespulmonary the patient had some desaturations to s ands with ambulation without improvement with oxygen withambulation but at this time the patient was completelyasymptomatic and the patients oxygen saturation recoveredspontaneously to the high s on room air with rest as aresult this was felt to possibly be not reflective of thepatients pulmonary status but reflective of some peripheralvascular changes with ambulation the patient was not felt toneed inpatient workup and will follow up with pcp as anoutpatienthematology the patient with acute blood loss anemiareceived a total of  units of packed red blood cells hadserial hematocrits while on heparin gtt and was transfused tokeep the hematocrit above musculoskeletal the patient was restarted on his prednisonefor polymyalgia rheumatica and temporal arteritisdischarge diagnoses  gastric ulcergastrointestinal bleeddemand ischemia elevated troponins and ekg changes in thesetting of acute blood loss anemiadischarge medications nitroglycerin sublingual prednisone  mg po daily atenolol  mg po q pm  mg po q am protonix  po bid aspirin  mg isosorbide mononitratedischarge followup  follow up with cardiology on last name lf   at  am the patients primary cardiologist is drlast name stitle  dr last name stitle  was not available so the patientfollowed up with dr last name stitle  the patient also followed up withdr last name stitle  for outpatient endoscopy on  at  pmand the patient was suggested to pursue cardiacrehabilitation in  to  weeks                        first name name pattern  last name namepattern  md number dictated bylast name namepattern medquistd   t   job  job number '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ9TzfxM4Rcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xzf biobert_v1.1_pubmed.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0nceCqxHssz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from pytorch_pretrained_bert.convert_tf_checkpoint_to_pytorch import convert_tf_checkpoint_to_pytorch\n",
        "path_bin = '/content/drive/My Drive/biobert_v1.1_pubmed_pytorch_model/pytorch_model.bin'\n",
        "path_bert = '/content/drive/My Drive/biobert_v1.1_pubmed/'\n",
        "\n",
        "if (not os.path.exists(path_bin)):\n",
        "  convert_tf_checkpoint_to_pytorch(\n",
        "  path_bert + \"biobert_model.ckpt\",\n",
        "  path_bert + \"bert_config.json\",\n",
        "  path_bert + \"pytorch_model.bin\"\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cY9_JCJm4nA",
        "colab_type": "code",
        "outputId": "8e35f1f0-9f28-4a34-ab2e-ba4947a6ed47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('biobert_v1.1_pubmed', do_lower_case=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykqok8NkDtch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_YmzDMWnX_d",
        "colab_type": "code",
        "outputId": "5f85c82a-6c93-4698-9b0c-a4c766f7d4be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences_hosp[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences_hosp[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences_hosp[0])))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:    the patient is a age over  yearold manwith a history of peptic ulcer disease coronary arterydisease status post myocardial infarction in  as well as temporal arteritis who presented with melenas  andchest pain the patient reported melanotic stools times since  pm on the day prior to admission no hematemesis orhematochezia stools were loose the patient had a history ofmelena in  the patient also reported beinglightheaded fatigued with an increase in his ch setdiscomfort for which he was taking sublingual nitroglycerinwith relief on the am of presentation the symptomspersisted the patient contactname ni  his pcp who sent him to thename ni  in the ed the p was found to have a hematocrit of decreased from a baseline of  to  he was given ivprotonix iv fluids and transfused the first of  units ofpacked red blood cells gastroenterology was consulted thepatient initially had an ekg with slight inferior changeswhile the patient was pain free the patient then had anepisode of  substernal chest pain in the ed with  to street address  changes in v to vpast medical history  significant for upper gastrointestinalbleed in  an esophagogastroduodenoscopy showed anulcer in the pylorus and chronic gastritis coronary arterydisease status post myocardial infarction in  and benign prostatic hypertrophy history of temporal arteritispemphigoid history of anemia history of small bowelvolvulus status post appendectomies status post inguinalhernia repair x history of colonic polyps and sigmoiddiverticulosisallergies  the patient has no known drug allergiesmedications  the patient was on celebrex aspirin prednisone atenolol imdur nitroglycerin prnsocial history  he is a retired physician name initial nameis remote tobacco history social alcohol use which isinfrequent married with  sonfamily history  noncontributoryphysical examination on admission as follows  vital signsvital signs of  temperature blood pressure  pulse respiratory rate of  and oxygen  on  litersgeneral the patient appeared comfortableheent examination was unremarkable except for paleconjunctiva dry mucosalaboratory data  significant for the hematocrit of  asstated above a potassium of  a bun  initial ck was with an mb of  and a troponin of  inr was urinalysis was unremarkable as stated above the patient had ekgs and the second of which showed  to street address  changes inv to v chest xray showed no acute cardiopulmonaryprocess so the patient was admitted to the hospitalconcise summary of hospital course as follows  gi thepatient was felt to likely have another bleeding ulcer as theetiology of his melanotic stools and anemia the patient hada history of helicobacter pylori in the past that wastreated the patient was felt to require egd to evaluate forrecurrent infection as well as ongoing bleeding the patientwas initially admitted to the icu gastroenterology wasconsulted the patient was taken for egd on  whichshowed a deep antral ulcer no acute bleeding the ulcer wasinjectedthe patient was initially continued on iv bid protonixhematocrits were followed and the patient was maintained on peripheral ivs at all times and aspirin was held thepatient has another episode of melanotic stool on he was taken for another egd at that time which showed theulcer was not bleeding as a result the patient was felt tobe stable for discharge to home from a gi perspective withcontinuation of the bid protonix the patient to follow upfor a repeat endoscopy in  weeks as an outpatientcardiac cardiac enzymes had been significant for elevatedtroponin on admission cardiology was contactname ni  who did notrecommend cardiac catheterization or coronary artery bypassgraft the patient initially received heparin and wasrestarted on aspirin which was approved by gi as long as thepatient had serial hematocrits the patient was transfused tokeep the hematocrit above  he was restarted on atenololthe patient was also on imdur for a longeracting vasodilatoreffect the patient had a couple of episodes of further chestpain during the admission but had no further ekg changespulmonary the patient had some desaturations to s ands with ambulation without improvement with oxygen withambulation but at this time the patient was completelyasymptomatic and the patients oxygen saturation recoveredspontaneously to the high s on room air with rest as aresult this was felt to possibly be not reflective of thepatients pulmonary status but reflective of some peripheralvascular changes with ambulation the patient was not felt toneed inpatient workup and will follow up with pcp as anoutpatienthematology the patient with acute blood loss anemiareceived a total of  units of packed red blood cells hadserial hematocrits while on heparin gtt and was transfused tokeep the hematocrit above musculoskeletal the patient was restarted on his prednisonefor polymyalgia rheumatica and temporal arteritisdischarge diagnoses  gastric ulcergastrointestinal bleeddemand ischemia elevated troponins and ekg changes in thesetting of acute blood loss anemiadischarge medications nitroglycerin sublingual prednisone  mg po daily atenolol  mg po q pm  mg po q am protonix  po bid aspirin  mg isosorbide mononitratedischarge followup  follow up with cardiology on last name lf   at  am the patients primary cardiologist is drlast name stitle  dr last name stitle  was not available so the patientfollowed up with dr last name stitle  the patient also followed up withdr last name stitle  for outpatient endoscopy on  at  pmand the patient was suggested to pursue cardiacrehabilitation in  to  weeks                        first name name pattern  last name namepattern  md number dictated bylast name namepattern medquistd   t   job  job number \n",
            "Tokenized:  ['the', 'patient', 'is', 'a', 'age', 'over', 'year', '##old', 'man', '##with', 'a', 'history', 'of', 'p', '##ept', '##ic', 'ul', '##cer', 'disease', 'co', '##rona', '##ry', 'artery', '##dis', '##ease', 'status', 'post', 'my', '##oc', '##ard', '##ial', 'in', '##far', '##ction', 'in', 'as', 'well', 'as', 'temporal', 'art', '##eri', '##tis', 'who', 'presented', 'with', 'me', '##lena', '##s', 'and', '##ches', '##t', 'pain', 'the', 'patient', 'reported', 'me', '##lan', '##otic', 'stool', '##s', 'times', 'since', 'pm', 'on', 'the', 'day', 'prior', 'to', 'admission', 'no', 'hem', '##ate', '##mes', '##is', 'or', '##hem', '##ato', '##che', '##zia', 'stool', '##s', 'were', 'loose', 'the', 'patient', 'had', 'a', 'history', 'of', '##mel', '##ena', 'in', 'the', 'patient', 'also', 'reported', 'being', '##light', '##headed', 'fatigue', '##d', 'with', 'an', 'increase', 'in', 'his', 'ch', 'set', '##dis', '##com', '##fort', 'for', 'which', 'he', 'was', 'taking', 'sub', '##ling', '##ual', 'ni', '##tro', '##gly', '##cer', '##in', '##with', 'relief', 'on', 'the', 'am', 'of', 'presentation', 'the', 'symptoms', '##pers', '##isted', 'the', 'patient', 'contact', '##name', 'ni', 'his', 'p', '##c', '##p', 'who', 'sent', 'him', 'to', 'then', '##ame', 'ni', 'in', 'the', 'ed', 'the', 'p', 'was', 'found', 'to', 'have', 'a', 'hem', '##ato', '##c', '##rit', 'of', 'decreased', 'from', 'a', 'base', '##line', 'of', 'to', 'he', 'was', 'given', 'i', '##v', '##p', '##rot', '##oni', '##x', 'i', '##v', 'fluids', 'and', 'trans', '##fused', 'the', 'first', 'of', 'units', 'of', '##pack', '##ed', 'red', 'blood', 'cells', 'gas', '##tro', '##enter', '##ology', 'was', 'consulted', 'the', '##patient', 'initially', 'had', 'an', 'e', '##k', '##g', 'with', 'slight', 'inferior', 'changes', '##while', 'the', 'patient', 'was', 'pain', 'free', 'the', 'patient', 'then', 'had', 'an', '##ep', '##is', '##ode', 'of', 'sub', '##ster', '##nal', 'chest', 'pain', 'in', 'the', 'ed', 'with', 'to', 'street', 'address', 'changes', 'in', 'v', 'to', 'v', '##pas', '##t', 'medical', 'history', 'significant', 'for', 'upper', 'gas', '##tro', '##int', '##est', '##inal', '##ble', '##ed', 'in', 'an', 'es', '##op', '##ha', '##go', '##gas', '##tro', '##du', '##ode', '##nos', '##copy', 'showed', 'an', '##ul', '##cer', 'in', 'the', 'p', '##yl', '##or', '##us', 'and', 'chronic', 'gas', '##tri', '##tis', 'co', '##rona', '##ry', 'artery', '##dis', '##ease', 'status', 'post', 'my', '##oc', '##ard', '##ial', 'in', '##far', '##ction', 'in', 'and', 'ben', '##ign', 'pro', '##static', 'h', '##yper', '##tro', '##phy', 'history', 'of', 'temporal', 'art', '##eri', '##tis', '##pe', '##mp', '##hi', '##go', '##id', 'history', 'of', 'an', '##emia', 'history', 'of', 'small', 'bow', '##el', '##vo', '##l', '##vu', '##lus', 'status', 'post', 'app', '##end', '##ec', '##tom', '##ies', 'status', 'post', 'ing', '##uin', '##al', '##her', '##nia', 'repair', 'x', 'history', 'of', 'co', '##lon', '##ic', 'p', '##oly', '##ps', 'and', 'si', '##g', '##mo', '##id', '##di', '##vert', '##ic', '##ulos', '##isa', '##ller', '##gies', 'the', 'patient', 'has', 'no', 'known', 'drug', 'all', '##er', '##gies', '##med', '##ica', '##tions', 'the', 'patient', 'was', 'on', 'c', '##ele', '##bre', '##x', 'as', '##pi', '##rin', 'pre', '##dn', '##ison', '##e', 'ate', '##no', '##lo', '##l', 'im', '##du', '##r', 'ni', '##tro', '##gly', '##cer', '##in', 'p', '##rns', '##oc', '##ial', 'history', 'he', 'is', 'a', 'retired', 'physician', 'name', 'initial', 'name', '##is', 'remote', 'tobacco', 'history', 'social', 'alcohol', 'use', 'which', 'is', '##in', '##f', '##re', '##quent', 'married', 'with', 'son', '##family', 'history', 'non', '##con', '##tri', '##but', '##ory', '##physical', 'examination', 'on', 'admission', 'as', 'follows', 'vital', 'signs', '##vi', '##tal', 'signs', 'of', 'temperature', 'blood', 'pressure', 'pulse', 'respiratory', 'rate', 'of', 'and', 'oxygen', 'on', 'liter', '##s', '##gene', '##ral', 'the', 'patient', 'appeared', 'comfortable', '##hee', '##nt', 'examination', 'was', 'un', '##rem', '##ark', '##able', 'except', 'for', 'pale', '##con', '##junct', '##iva', 'dry', 'm', '##uc', '##osa', '##la', '##bor', '##atory', 'data', 'significant', 'for', 'the', 'hem', '##ato', '##c', '##rit', 'of', 'ass', '##tated', 'above', 'a', 'potassium', 'of', 'a', 'b', '##un', 'initial', 'c', '##k', 'was', 'with', 'an', 'm', '##b', 'of', 'and', 'a', 't', '##rop', '##oni', '##n', 'of', 'in', '##r', 'was', 'u', '##rina', '##lysis', 'was', 'un', '##rem', '##ark', '##able', 'as', 'stated', 'above', 'the', 'patient', 'had', 'e', '##k', '##gs', 'and', 'the', 'second', 'of', 'which', 'showed', 'to', 'street', 'address', 'changes', 'in', '##v', 'to', 'v', 'chest', 'x', '##ray', 'showed', 'no', 'acute', 'card', '##io', '##pu', '##lm', '##ona', '##ry', '##p', '##ro', '##cess', 'so', 'the', 'patient', 'was', 'admitted', 'to', 'the', 'hospital', '##con', '##cise', 'summary', 'of', 'hospital', 'course', 'as', 'follows', 'g', '##i', 'the', '##patient', 'was', 'felt', 'to', 'likely', 'have', 'another', 'bleeding', 'ul', '##cer', 'as', 'thee', '##ti', '##ology', 'of', 'his', 'me', '##lan', '##otic', 'stool', '##s', 'and', 'an', '##emia', 'the', 'patient', 'had', '##a', 'history', 'of', 'he', '##lic', '##ob', '##act', '##er', 'p', '##yl', '##ori', 'in', 'the', 'past', 'that', 'was', '##tre', '##ated', 'the', 'patient', 'was', 'felt', 'to', 'require', 'e', '##g', '##d', 'to', 'evaluate', 'for', '##re', '##current', 'infection', 'as', 'well', 'as', 'ongoing', 'bleeding', 'the', 'patient', '##was', 'initially', 'admitted', 'to', 'the', 'i', '##cu', 'gas', '##tro', '##enter', '##ology', 'was', '##con', '##sul', '##ted', 'the', 'patient', 'was', 'taken', 'for', 'e', '##g', '##d', 'on', 'which', '##sh', '##owed', 'a', 'deep', 'ant', '##ral', 'ul', '##cer', 'no', 'acute', 'bleeding', 'the', 'ul', '##cer', 'was', '##in', '##jected', '##the', 'patient', 'was', 'initially', 'continued', 'on', 'i', '##v', 'bid', 'pro', '##ton', '##ix', '##hem', '##ato', '##c', '##rits', 'were', 'followed', 'and', 'the', 'patient', 'was', 'maintained', 'on', 'peripheral', 'i', '##v', '##s', 'at', 'all', 'times', 'and', 'as', '##pi', '##rin', 'was', 'held', 'the', '##patient', 'has', 'another', 'episode', 'of', 'me', '##lan', '##otic', 'stool', 'on', 'he', 'was', 'taken', 'for', 'another', 'e', '##g', '##d', 'at', 'that', 'time', 'which', 'showed', 'the', '##ul', '##cer', 'was', 'not', 'bleeding', 'as', 'a', 'result', 'the', 'patient', 'was', 'felt', 'to', '##be', 'stable', 'for', 'discharge', 'to', 'home', 'from', 'a', 'g', '##i', 'perspective', 'with', '##con', '##tin', '##uation', 'of', 'the', 'bid', 'pro', '##ton', '##ix', 'the', 'patient', 'to', 'follow', 'up', '##fo', '##r', 'a', 'repeat', 'end', '##os', '##copy', 'in', 'weeks', 'as', 'an', 'out', '##patient', '##card', '##iac', 'cardiac', 'enzymes', 'had', 'been', 'significant', 'for', 'elevated', '##tro', '##po', '##nin', 'on', 'admission', 'card', '##iology', 'was', 'contact', '##name', 'ni', 'who', 'did', 'not', '##re', '##com', '##men', '##d', 'cardiac', 'cat', '##he', '##ter', '##ization', 'or', 'co', '##rona', '##ry', 'artery', 'bypass', '##gra', '##ft', 'the', 'patient', 'initially', 'received', 'he', '##par', '##in', 'and', 'was', '##rest', '##arte', '##d', 'on', 'as', '##pi', '##rin', 'which', 'was', 'approved', 'by', 'g', '##i', 'as', 'long', 'as', 'the', '##patient', 'had', 'serial', 'hem', '##ato', '##c', '##rits', 'the', 'patient', 'was', 'trans', '##fused', 'to', '##ke', '##ep', 'the', 'hem', '##ato', '##c', '##rit', 'above', 'he', 'was', 'restart', '##ed', 'on', 'ate', '##no', '##lo', '##lt', '##he', 'patient', 'was', 'also', 'on', 'im', '##du', '##r', 'for', 'a', 'longer', '##act', '##ing', 'v', '##as', '##od', '##ila', '##tore', '##ffe', '##ct', 'the', 'patient', 'had', 'a', 'couple', 'of', 'episodes', 'of', 'further', 'chest', '##pa', '##in', 'during', 'the', 'admission', 'but', 'had', 'no', 'further', 'e', '##k', '##g', 'changes', '##pu', '##lm', '##ona', '##ry', 'the', 'patient', 'had', 'some', 'des', '##at', '##uration', '##s', 'to', 's', 'and', '##s', 'with', 'am', '##bula', '##tion', 'without', 'improvement', 'with', 'oxygen', 'with', '##am', '##bula', '##tion', 'but', 'at', 'this', 'time', 'the', 'patient', 'was', 'completely', '##as', '##ym', '##pt', '##oma', '##tic', 'and', 'the', 'patients', 'oxygen', 'sat', '##uration', 'recovered', '##sp', '##ont', '##aneous', '##ly', 'to', 'the', 'high', 's', 'on', 'room', 'air', 'with', 'rest', 'as', 'are', '##sul', '##t', 'this', 'was', 'felt', 'to', 'possibly', 'be', 'not', 'reflective', 'of', 'the', '##patient', '##s', 'pulmonary', 'status', 'but', 'reflective', 'of', 'some', 'peripheral', '##vas', '##cular', 'changes', 'with', 'am', '##bula', '##tion', 'the', 'patient', 'was', 'not', 'felt', 'tone', '##ed', 'in', '##patient', 'work', '##up', 'and', 'will', 'follow', 'up', 'with', 'p', '##c', '##p', 'as', 'an', '##out', '##patient', '##hem', '##ato', '##logy', 'the', 'patient', 'with', 'acute', 'blood', 'loss', 'an', '##emia', '##re', '##ceived', 'a', 'total', 'of', 'units', 'of', 'packed', 'red', 'blood', 'cells', 'had', '##ser', '##ial', 'hem', '##ato', '##c', '##rits', 'while', 'on', 'he', '##par', '##in', 'g', '##tt', 'and', 'was', 'trans', '##fused', 'to', '##ke', '##ep', 'the', 'hem', '##ato', '##c', '##rit', 'above', 'm', '##us', '##cu', '##los', '##kel', '##etal', 'the', 'patient', 'was', 'restart', '##ed', 'on', 'his', 'pre', '##dn', '##ison', '##ef', '##or', 'p', '##oly', '##my', '##al', '##gia', 'r', '##he', '##umatic', '##a', 'and', 'temporal', 'art', '##eri', '##tis', '##dis', '##cha', '##rge', 'di', '##ag', '##nose', '##s', 'gas', '##tric', 'ul', '##cer', '##gas', '##tro', '##int', '##est', '##inal', 'bleed', '##de', '##mand', 'is', '##che', '##mia', 'elevated', 't', '##rop', '##oni', '##ns', 'and', 'e', '##k', '##g', 'changes', 'in', 'these', '##tting', 'of', 'acute', 'blood', 'loss', 'an', '##emia', '##dis', '##cha', '##rge', 'medications', 'ni', '##tro', '##gly', '##cer', '##in', 'sub', '##ling', '##ual', 'pre', '##dn', '##ison', '##e', 'mg', 'p', '##o', 'daily', 'ate', '##no', '##lo', '##l', 'mg', 'p', '##o', 'q', 'pm', 'mg', 'p', '##o', 'q', 'am', 'pro', '##ton', '##ix', 'p', '##o', 'bid', 'as', '##pi', '##rin', 'mg', 'is', '##oso', '##rb', '##ide', 'mon', '##oni', '##trate', '##dis', '##cha', '##rge', 'follow', '##up', 'follow', 'up', 'with', 'card', '##iology', 'on', 'last', 'name', 'l', '##f', 'at', 'am', 'the', 'patients', 'primary', 'card', '##iol', '##ogist', 'is', 'd', '##rl', '##ast', 'name', 's', '##ti', '##tle', 'd', '##r', 'last', 'name', 's', '##ti', '##tle', 'was', 'not', 'available', 'so', 'the', 'patient', '##fo', '##llow', '##ed', 'up', 'with', 'd', '##r', 'last', 'name', 's', '##ti', '##tle', 'the', 'patient', 'also', 'followed', 'up', 'with', '##dr', 'last', 'name', 's', '##ti', '##tle', 'for', 'out', '##patient', 'end', '##os', '##copy', 'on', 'at', 'pm', '##and', 'the', 'patient', 'was', 'suggested', 'to', 'pursue', 'cardiac', '##re', '##habilitation', 'in', 'to', 'weeks', 'first', 'name', 'name', 'pattern', 'last', 'name', 'name', '##pa', '##tter', '##n', 'm', '##d', 'number', 'dictated', 'by', '##last', 'name', 'name', '##pa', '##tter', '##n', 'me', '##d', '##quist', '##d', 't', 'job', 'job', 'number']\n",
            "Token IDs:  [1103, 5351, 1110, 170, 1425, 1166, 1214, 11015, 1299, 22922, 170, 1607, 1104, 185, 15384, 1596, 23449, 14840, 3653, 1884, 15789, 1616, 18593, 10396, 23860, 2781, 2112, 1139, 13335, 2881, 2916, 1107, 14794, 5796, 1107, 1112, 1218, 1112, 18107, 1893, 9866, 6620, 1150, 2756, 1114, 1143, 23675, 1116, 1105, 7486, 1204, 2489, 1103, 5351, 2103, 1143, 4371, 14426, 15631, 1116, 1551, 1290, 9852, 1113, 1103, 1285, 2988, 1106, 10296, 1185, 23123, 2193, 6801, 1548, 1137, 15391, 10024, 4386, 12432, 15631, 1116, 1127, 5768, 1103, 5351, 1125, 170, 1607, 1104, 10212, 7076, 1107, 1103, 5351, 1145, 2103, 1217, 4568, 14443, 18418, 1181, 1114, 1126, 2773, 1107, 1117, 22572, 1383, 10396, 8178, 11088, 1111, 1134, 1119, 1108, 1781, 4841, 1979, 4746, 11437, 8005, 18901, 14840, 1394, 22922, 3893, 1113, 1103, 1821, 1104, 8685, 1103, 8006, 6206, 20330, 1103, 5351, 3232, 16124, 11437, 1117, 185, 1665, 1643, 1150, 1850, 1140, 1106, 1173, 16470, 11437, 1107, 1103, 5048, 1103, 185, 1108, 1276, 1106, 1138, 170, 23123, 10024, 1665, 7729, 1104, 10558, 1121, 170, 2259, 2568, 1104, 1106, 1119, 1108, 1549, 178, 1964, 1643, 10595, 11153, 1775, 178, 1964, 24024, 1105, 14715, 21089, 1103, 1148, 1104, 2338, 1104, 27569, 1174, 1894, 1892, 3652, 3245, 8005, 25195, 4807, 1108, 18881, 1103, 27420, 2786, 1125, 1126, 174, 1377, 1403, 1114, 6812, 15543, 2607, 21053, 1103, 5351, 1108, 2489, 1714, 1103, 5351, 1173, 1125, 1126, 8043, 1548, 13040, 1104, 4841, 4648, 7050, 2229, 2489, 1107, 1103, 5048, 1114, 1106, 2472, 4134, 2607, 1107, 191, 1106, 191, 22939, 1204, 2657, 1607, 2418, 1111, 3105, 3245, 8005, 10879, 2556, 14196, 2165, 1174, 1107, 1126, 13936, 4184, 2328, 2758, 11305, 8005, 7641, 13040, 14226, 20739, 2799, 1126, 4654, 14840, 1107, 1103, 185, 7777, 1766, 1361, 1105, 13306, 3245, 19091, 6620, 1884, 15789, 1616, 18593, 10396, 23860, 2781, 2112, 1139, 13335, 2881, 2916, 1107, 14794, 5796, 1107, 1105, 26181, 11368, 5250, 27372, 177, 24312, 8005, 22192, 1607, 1104, 18107, 1893, 9866, 6620, 3186, 8223, 3031, 2758, 2386, 1607, 1104, 1126, 20504, 1607, 1104, 1353, 7125, 1883, 6005, 1233, 25247, 5954, 2781, 2112, 12647, 6696, 10294, 18778, 1905, 2781, 2112, 16664, 14846, 1348, 4679, 5813, 6949, 193, 1607, 1104, 1884, 4934, 1596, 185, 23415, 3491, 1105, 27466, 1403, 3702, 2386, 3309, 12986, 1596, 24153, 15630, 9860, 19310, 1103, 5351, 1144, 1185, 1227, 3850, 1155, 1200, 19310, 4611, 4578, 6126, 1103, 5351, 1108, 1113, 172, 11194, 9730, 1775, 1112, 8508, 4854, 3073, 22834, 7614, 1162, 8756, 2728, 2858, 1233, 13280, 7641, 1197, 11437, 8005, 18901, 14840, 1394, 185, 20163, 13335, 2916, 1607, 1119, 1110, 170, 2623, 7454, 1271, 3288, 1271, 1548, 6456, 10468, 1607, 1934, 6272, 1329, 1134, 1110, 1394, 2087, 1874, 14855, 1597, 1114, 1488, 21390, 1607, 1664, 7235, 19091, 16442, 4649, 18547, 8179, 1113, 10296, 1112, 3226, 9301, 5300, 5086, 6163, 5300, 1104, 4143, 1892, 2997, 8561, 19192, 2603, 1104, 1105, 7621, 1113, 27146, 1116, 27054, 4412, 1103, 5351, 1691, 6062, 19989, 2227, 8179, 1108, 8362, 16996, 23822, 1895, 2589, 1111, 4554, 7235, 20327, 12416, 3712, 182, 21977, 9275, 1742, 12207, 13424, 2233, 2418, 1111, 1103, 23123, 10024, 1665, 7729, 1104, 3919, 20213, 1807, 170, 21177, 1104, 170, 171, 3488, 3288, 172, 1377, 1108, 1114, 1126, 182, 1830, 1104, 1105, 170, 189, 12736, 11153, 1179, 1104, 1107, 1197, 1108, 190, 9324, 15140, 1108, 8362, 16996, 23822, 1895, 1112, 2202, 1807, 1103, 5351, 1125, 174, 1377, 5700, 1105, 1103, 1248, 1104, 1134, 2799, 1106, 2472, 4134, 2607, 1107, 1964, 1106, 191, 2229, 193, 6447, 2799, 1185, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 1643, 2180, 22371, 1177, 1103, 5351, 1108, 4120, 1106, 1103, 2704, 7235, 14636, 14940, 1104, 2704, 1736, 1112, 3226, 176, 1182, 1103, 27420, 1108, 1464, 1106, 2620, 1138, 1330, 9793, 23449, 14840, 1112, 20021, 3121, 4807, 1104, 1117, 1143, 4371, 14426, 15631, 1116, 1105, 1126, 20504, 1103, 5351, 1125, 1161, 1607, 1104, 1119, 8031, 12809, 11179, 1200, 185, 7777, 9012, 1107, 1103, 1763, 1115, 1108, 7877, 2913, 1103, 5351, 1108, 1464, 1106, 4752, 174, 1403, 1181, 1106, 17459, 1111, 1874, 21754, 8974, 1112, 1218, 1112, 7173, 9793, 1103, 5351, 20963, 2786, 4120, 1106, 1103, 178, 10182, 3245, 8005, 25195, 4807, 1108, 7235, 24661, 1906, 1103, 5351, 1108, 1678, 1111, 174, 1403, 1181, 1113, 1134, 2737, 12595, 170, 1996, 22904, 4412, 23449, 14840, 1185, 12104, 9793, 1103, 23449, 14840, 1108, 1394, 24653, 10681, 5351, 1108, 2786, 1598, 1113, 178, 1964, 6875, 5250, 1633, 7231, 15391, 10024, 1665, 26846, 1127, 1723, 1105, 1103, 5351, 1108, 4441, 1113, 17963, 178, 1964, 1116, 1120, 1155, 1551, 1105, 1112, 8508, 4854, 1108, 1316, 1103, 27420, 1144, 1330, 2004, 1104, 1143, 4371, 14426, 15631, 1113, 1119, 1108, 1678, 1111, 1330, 174, 1403, 1181, 1120, 1115, 1159, 1134, 2799, 1103, 4654, 14840, 1108, 1136, 9793, 1112, 170, 1871, 1103, 5351, 1108, 1464, 1106, 3962, 6111, 1111, 12398, 1106, 1313, 1121, 170, 176, 1182, 7281, 1114, 7235, 6105, 10255, 1104, 1103, 6875, 5250, 1633, 7231, 1103, 5351, 1106, 2812, 1146, 14467, 1197, 170, 9488, 1322, 2155, 20739, 1107, 2277, 1112, 1126, 1149, 27420, 10542, 12571, 17688, 17664, 1125, 1151, 2418, 1111, 8208, 8005, 5674, 10430, 1113, 10296, 3621, 17288, 1108, 3232, 16124, 11437, 1150, 1225, 1136, 1874, 8178, 2354, 1181, 17688, 5855, 4638, 2083, 2734, 1137, 1884, 15789, 1616, 18593, 13981, 14867, 4964, 1103, 5351, 2786, 1460, 1119, 17482, 1394, 1105, 1108, 14201, 26579, 1181, 1113, 1112, 8508, 4854, 1134, 1108, 4092, 1118, 176, 1182, 1112, 1263, 1112, 1103, 27420, 1125, 7838, 23123, 10024, 1665, 26846, 1103, 5351, 1108, 14715, 21089, 1106, 2391, 8043, 1103, 23123, 10024, 1665, 7729, 1807, 1119, 1108, 27777, 1174, 1113, 8756, 2728, 2858, 6066, 4638, 5351, 1108, 1145, 1113, 13280, 7641, 1197, 1111, 170, 2039, 11179, 1158, 191, 2225, 5412, 8009, 24612, 15475, 5822, 1103, 5351, 1125, 170, 2337, 1104, 3426, 1104, 1748, 2229, 4163, 1394, 1219, 1103, 10296, 1133, 1125, 1185, 1748, 174, 1377, 1403, 2607, 16091, 13505, 7637, 1616, 1103, 5351, 1125, 1199, 3532, 2980, 23022, 1116, 1106, 188, 1105, 1116, 1114, 1821, 23601, 2116, 1443, 8331, 1114, 7621, 1114, 2312, 23601, 2116, 1133, 1120, 1142, 1159, 1103, 5351, 1108, 2423, 2225, 17162, 6451, 7903, 2941, 1105, 1103, 4420, 7621, 2068, 23022, 6203, 20080, 9921, 13064, 1193, 1106, 1103, 1344, 188, 1113, 1395, 1586, 1114, 1832, 1112, 1132, 24661, 1204, 1142, 1108, 1464, 1106, 3566, 1129, 1136, 24449, 1104, 1103, 27420, 1116, 26600, 2781, 1133, 24449, 1104, 1199, 17963, 11509, 11702, 2607, 1114, 1821, 23601, 2116, 1103, 5351, 1108, 1136, 1464, 3586, 1174, 1107, 27420, 1250, 4455, 1105, 1209, 2812, 1146, 1114, 185, 1665, 1643, 1112, 1126, 3554, 27420, 15391, 10024, 6360, 1103, 5351, 1114, 12104, 1892, 2445, 1126, 20504, 1874, 21437, 170, 1703, 1104, 2338, 1104, 8733, 1894, 1892, 3652, 1125, 6906, 2916, 23123, 10024, 1665, 26846, 1229, 1113, 1119, 17482, 1394, 176, 3069, 1105, 1108, 14715, 21089, 1106, 2391, 8043, 1103, 23123, 10024, 1665, 7729, 1807, 182, 1361, 10182, 8867, 13622, 21470, 1103, 5351, 1108, 27777, 1174, 1113, 1117, 3073, 22834, 7614, 11470, 1766, 185, 23415, 4527, 1348, 9037, 187, 4638, 25930, 1161, 1105, 18107, 1893, 9866, 6620, 10396, 7147, 12272, 4267, 8517, 22583, 1116, 3245, 11048, 23449, 14840, 11305, 8005, 10879, 2556, 14196, 24752, 2007, 20993, 1110, 4386, 8191, 8208, 189, 12736, 11153, 2316, 1105, 174, 1377, 1403, 2607, 1107, 1292, 19162, 1104, 12104, 1892, 2445, 1126, 20504, 10396, 7147, 12272, 23897, 11437, 8005, 18901, 14840, 1394, 4841, 1979, 4746, 3073, 22834, 7614, 1162, 17713, 185, 1186, 3828, 8756, 2728, 2858, 1233, 17713, 185, 1186, 186, 9852, 17713, 185, 1186, 186, 1821, 5250, 1633, 7231, 185, 1186, 6875, 1112, 8508, 4854, 17713, 1110, 22354, 26281, 3269, 19863, 11153, 18775, 10396, 7147, 12272, 2812, 4455, 2812, 1146, 1114, 3621, 17288, 1113, 1314, 1271, 181, 2087, 1120, 1821, 1103, 4420, 2425, 3621, 19840, 25976, 1110, 173, 17670, 12788, 1271, 188, 3121, 5034, 173, 1197, 1314, 1271, 188, 3121, 5034, 1108, 1136, 1907, 1177, 1103, 5351, 14467, 24834, 1174, 1146, 1114, 173, 1197, 1314, 1271, 188, 3121, 5034, 1103, 5351, 1145, 1723, 1146, 1114, 23632, 1314, 1271, 188, 3121, 5034, 1111, 1149, 27420, 1322, 2155, 20739, 1113, 1120, 9852, 5709, 1103, 5351, 1108, 3228, 1106, 6799, 17688, 1874, 27778, 1107, 1106, 2277, 1148, 1271, 1271, 4844, 1314, 1271, 1271, 4163, 8634, 1179, 182, 1181, 1295, 26754, 1118, 19268, 1271, 1271, 4163, 8634, 1179, 1143, 1181, 19854, 1181, 189, 2261, 2261, 1295]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSaPc0nIoUNU",
        "colab_type": "code",
        "outputId": "3671a007-31ef-4a7a-9b8d-79a5502fae9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences_hosp:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 512,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences_hosp[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:    the patient is a age over  yearold manwith a history of peptic ulcer disease coronary arterydisease status post myocardial infarction in  as well as temporal arteritis who presented with melenas  andchest pain the patient reported melanotic stools times since  pm on the day prior to admission no hematemesis orhematochezia stools were loose the patient had a history ofmelena in  the patient also reported beinglightheaded fatigued with an increase in his ch setdiscomfort for which he was taking sublingual nitroglycerinwith relief on the am of presentation the symptomspersisted the patient contactname ni  his pcp who sent him to thename ni  in the ed the p was found to have a hematocrit of decreased from a baseline of  to  he was given ivprotonix iv fluids and transfused the first of  units ofpacked red blood cells gastroenterology was consulted thepatient initially had an ekg with slight inferior changeswhile the patient was pain free the patient then had anepisode of  substernal chest pain in the ed with  to street address  changes in v to vpast medical history  significant for upper gastrointestinalbleed in  an esophagogastroduodenoscopy showed anulcer in the pylorus and chronic gastritis coronary arterydisease status post myocardial infarction in  and benign prostatic hypertrophy history of temporal arteritispemphigoid history of anemia history of small bowelvolvulus status post appendectomies status post inguinalhernia repair x history of colonic polyps and sigmoiddiverticulosisallergies  the patient has no known drug allergiesmedications  the patient was on celebrex aspirin prednisone atenolol imdur nitroglycerin prnsocial history  he is a retired physician name initial nameis remote tobacco history social alcohol use which isinfrequent married with  sonfamily history  noncontributoryphysical examination on admission as follows  vital signsvital signs of  temperature blood pressure  pulse respiratory rate of  and oxygen  on  litersgeneral the patient appeared comfortableheent examination was unremarkable except for paleconjunctiva dry mucosalaboratory data  significant for the hematocrit of  asstated above a potassium of  a bun  initial ck was with an mb of  and a troponin of  inr was urinalysis was unremarkable as stated above the patient had ekgs and the second of which showed  to street address  changes inv to v chest xray showed no acute cardiopulmonaryprocess so the patient was admitted to the hospitalconcise summary of hospital course as follows  gi thepatient was felt to likely have another bleeding ulcer as theetiology of his melanotic stools and anemia the patient hada history of helicobacter pylori in the past that wastreated the patient was felt to require egd to evaluate forrecurrent infection as well as ongoing bleeding the patientwas initially admitted to the icu gastroenterology wasconsulted the patient was taken for egd on  whichshowed a deep antral ulcer no acute bleeding the ulcer wasinjectedthe patient was initially continued on iv bid protonixhematocrits were followed and the patient was maintained on peripheral ivs at all times and aspirin was held thepatient has another episode of melanotic stool on he was taken for another egd at that time which showed theulcer was not bleeding as a result the patient was felt tobe stable for discharge to home from a gi perspective withcontinuation of the bid protonix the patient to follow upfor a repeat endoscopy in  weeks as an outpatientcardiac cardiac enzymes had been significant for elevatedtroponin on admission cardiology was contactname ni  who did notrecommend cardiac catheterization or coronary artery bypassgraft the patient initially received heparin and wasrestarted on aspirin which was approved by gi as long as thepatient had serial hematocrits the patient was transfused tokeep the hematocrit above  he was restarted on atenololthe patient was also on imdur for a longeracting vasodilatoreffect the patient had a couple of episodes of further chestpain during the admission but had no further ekg changespulmonary the patient had some desaturations to s ands with ambulation without improvement with oxygen withambulation but at this time the patient was completelyasymptomatic and the patients oxygen saturation recoveredspontaneously to the high s on room air with rest as aresult this was felt to possibly be not reflective of thepatients pulmonary status but reflective of some peripheralvascular changes with ambulation the patient was not felt toneed inpatient workup and will follow up with pcp as anoutpatienthematology the patient with acute blood loss anemiareceived a total of  units of packed red blood cells hadserial hematocrits while on heparin gtt and was transfused tokeep the hematocrit above musculoskeletal the patient was restarted on his prednisonefor polymyalgia rheumatica and temporal arteritisdischarge diagnoses  gastric ulcergastrointestinal bleeddemand ischemia elevated troponins and ekg changes in thesetting of acute blood loss anemiadischarge medications nitroglycerin sublingual prednisone  mg po daily atenolol  mg po q pm  mg po q am protonix  po bid aspirin  mg isosorbide mononitratedischarge followup  follow up with cardiology on last name lf   at  am the patients primary cardiologist is drlast name stitle  dr last name stitle  was not available so the patientfollowed up with dr last name stitle  the patient also followed up withdr last name stitle  for outpatient endoscopy on  at  pmand the patient was suggested to pursue cardiacrehabilitation in  to  weeks                        first name name pattern  last name namepattern  md number dictated bylast name namepattern medquistd   t   job  job number \n",
            "Token IDs: tensor([  101,  1103,  5351,  1110,   170,  1425,  1166,  1214, 11015,  1299,\n",
            "        22922,   170,  1607,  1104,   185, 15384,  1596, 23449, 14840,  3653,\n",
            "         1884, 15789,  1616, 18593, 10396, 23860,  2781,  2112,  1139, 13335,\n",
            "         2881,  2916,  1107, 14794,  5796,  1107,  1112,  1218,  1112, 18107,\n",
            "         1893,  9866,  6620,  1150,  2756,  1114,  1143, 23675,  1116,  1105,\n",
            "         7486,  1204,  2489,  1103,  5351,  2103,  1143,  4371, 14426, 15631,\n",
            "         1116,  1551,  1290,  9852,  1113,  1103,  1285,  2988,  1106, 10296,\n",
            "         1185, 23123,  2193,  6801,  1548,  1137, 15391, 10024,  4386, 12432,\n",
            "        15631,  1116,  1127,  5768,  1103,  5351,  1125,   170,  1607,  1104,\n",
            "        10212,  7076,  1107,  1103,  5351,  1145,  2103,  1217,  4568, 14443,\n",
            "        18418,  1181,  1114,  1126,  2773,  1107,  1117, 22572,  1383, 10396,\n",
            "         8178, 11088,  1111,  1134,  1119,  1108,  1781,  4841,  1979,  4746,\n",
            "        11437,  8005, 18901, 14840,  1394, 22922,  3893,  1113,  1103,  1821,\n",
            "         1104,  8685,  1103,  8006,  6206, 20330,  1103,  5351,  3232, 16124,\n",
            "        11437,  1117,   185,  1665,  1643,  1150,  1850,  1140,  1106,  1173,\n",
            "        16470, 11437,  1107,  1103,  5048,  1103,   185,  1108,  1276,  1106,\n",
            "         1138,   170, 23123, 10024,  1665,  7729,  1104, 10558,  1121,   170,\n",
            "         2259,  2568,  1104,  1106,  1119,  1108,  1549,   178,  1964,  1643,\n",
            "        10595, 11153,  1775,   178,  1964, 24024,  1105, 14715, 21089,  1103,\n",
            "         1148,  1104,  2338,  1104, 27569,  1174,  1894,  1892,  3652,  3245,\n",
            "         8005, 25195,  4807,  1108, 18881,  1103, 27420,  2786,  1125,  1126,\n",
            "          174,  1377,  1403,  1114,  6812, 15543,  2607, 21053,  1103,  5351,\n",
            "         1108,  2489,  1714,  1103,  5351,  1173,  1125,  1126,  8043,  1548,\n",
            "        13040,  1104,  4841,  4648,  7050,  2229,  2489,  1107,  1103,  5048,\n",
            "         1114,  1106,  2472,  4134,  2607,  1107,   191,  1106,   191, 22939,\n",
            "         1204,  2657,  1607,  2418,  1111,  3105,  3245,  8005, 10879,  2556,\n",
            "        14196,  2165,  1174,  1107,  1126, 13936,  4184,  2328,  2758, 11305,\n",
            "         8005,  7641, 13040, 14226, 20739,  2799,  1126,  4654, 14840,  1107,\n",
            "         1103,   185,  7777,  1766,  1361,  1105, 13306,  3245, 19091,  6620,\n",
            "         1884, 15789,  1616, 18593, 10396, 23860,  2781,  2112,  1139, 13335,\n",
            "         2881,  2916,  1107, 14794,  5796,  1107,  1105, 26181, 11368,  5250,\n",
            "        27372,   177, 24312,  8005, 22192,  1607,  1104, 18107,  1893,  9866,\n",
            "         6620,  3186,  8223,  3031,  2758,  2386,  1607,  1104,  1126, 20504,\n",
            "         1607,  1104,  1353,  7125,  1883,  6005,  1233, 25247,  5954,  2781,\n",
            "         2112, 12647,  6696, 10294, 18778,  1905,  2781,  2112, 16664, 14846,\n",
            "         1348,  4679,  5813,  6949,   193,  1607,  1104,  1884,  4934,  1596,\n",
            "          185, 23415,  3491,  1105, 27466,  1403,  3702,  2386,  3309, 12986,\n",
            "         1596, 24153, 15630,  9860, 19310,  1103,  5351,  1144,  1185,  1227,\n",
            "         3850,  1155,  1200, 19310,  4611,  4578,  6126,  1103,  5351,  1108,\n",
            "         1113,   172, 11194,  9730,  1775,  1112,  8508,  4854,  3073, 22834,\n",
            "         7614,  1162,  8756,  2728,  2858,  1233, 13280,  7641,  1197, 11437,\n",
            "         8005, 18901, 14840,  1394,   185, 20163, 13335,  2916,  1607,  1119,\n",
            "         1110,   170,  2623,  7454,  1271,  3288,  1271,  1548,  6456, 10468,\n",
            "         1607,  1934,  6272,  1329,  1134,  1110,  1394,  2087,  1874, 14855,\n",
            "         1597,  1114,  1488, 21390,  1607,  1664,  7235, 19091, 16442,  4649,\n",
            "        18547,  8179,  1113, 10296,  1112,  3226,  9301,  5300,  5086,  6163,\n",
            "         5300,  1104,  4143,  1892,  2997,  8561, 19192,  2603,  1104,  1105,\n",
            "         7621,  1113, 27146,  1116, 27054,  4412,  1103,  5351,  1691,  6062,\n",
            "        19989,  2227,  8179,  1108,  8362, 16996, 23822,  1895,  2589,  1111,\n",
            "         4554,  7235, 20327, 12416,  3712,   182, 21977,  9275,  1742, 12207,\n",
            "        13424,  2233,  2418,  1111,  1103, 23123, 10024,  1665,  7729,  1104,\n",
            "         3919,   102])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO9Cc7JzpGYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                          random_state=2018, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1i47ATwpH3b",
        "colab_type": "code",
        "outputId": "49f8d645-a336-4c36-ce5e-e5ffc30a315b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9DA_TzVuIRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 12\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8IIMmK8egfe",
        "colab_type": "code",
        "outputId": "e1f011d1-3e67-4753-f6a1-34271da3bfce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained('/content/drive/My Drive/biobert_v1.1_pubmed_pytorch_model/', num_labels=10)\n",
        "model.cuda()\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFKkd8IzXl4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01}\n",
        "]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbvWzJVzsoJa",
        "colab_type": "code",
        "outputId": "7f32037f-66a8-40ca-ecd0-4020150e4b5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=1e-5,\n",
        "                     warmup=.1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t_total value of -1 results in schedule not being applied\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgBwm_jlspqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "errors = {\"0\": 0, \"1\":0, \"2\":0, \"3\":0, \"4\":0, \"5\":0, \"6\":0, \"7\":0, \"8\":0, \"9\":0}\n",
        "total = {\"0\": 0, \"1\":0, \"2\":0, \"3\":0, \"4\":0, \"5\":0, \"6\":0, \"7\":0, \"8\":0, \"9\":0}\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    print('-----------------------')\n",
        "    print('pred: ', pred_flat)\n",
        "    labels_flat = labels.flatten()\n",
        "    print('labels: ', labels_flat)\n",
        "    \n",
        "    ##determine frequent errors\n",
        "    for i in range(len(pred_flat)):\n",
        "      if labels_flat[i] != pred_flat[i]:\n",
        "        errors[str(labels_flat[i])] += 1\n",
        "      total[str(labels_flat[i])] += 1\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf_TCwwIsrcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "\n",
        "def aucscore(labels, preds):\n",
        "  print(preds)\n",
        "  pred_flat = np.max(preds, axis=1).flatten()\n",
        "  print(pred_flat)\n",
        "  labels_flat = labels\n",
        "  print('x')\n",
        "  micro = roc_auc_score(labels_flat, pred_flat, average='micro', multi_class='ovo')\n",
        "  print('x')\n",
        "  macro = roc_auc_score(labels_flat, pred_flat, average='macro', multi_class='ovo')\n",
        "  weighted = roc_auc_score(labels_flat, pred_flat, average='weighted', multi_class='ovo')\n",
        "\n",
        "  return micro, macro, weighted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6vAE-CGsthy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1score(labels, preds):\n",
        "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "\n",
        "  micro = f1_score(labels_flat, pred_flat, average='micro')\n",
        "  macro = f1_score(labels_flat, pred_flat, average='macro')\n",
        "  weighted = f1_score(labels_flat, pred_flat, average='weighted')\n",
        "\n",
        "  return micro, macro, weighted\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi-DAKDysvLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG9KJTzvsw13",
        "colab_type": "code",
        "outputId": "b5c7a3e7-90e7-401d-904b-74caa4a9744d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "  \n",
        "  \n",
        "  # Training\n",
        "  \n",
        "  # Set our model to training mode (as opposed to evaluation mode)\n",
        "  model.train()\n",
        "  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  total_eval_loss=0\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "\n",
        "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "    \n",
        "  # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  f1_micr, f1_macr, f1_weight = 0,0,0\n",
        "  auc_micr, auc_macr, auc_weight = 0,0,0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        " \n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    \n",
        "    total_eval_loss+= loss.item()\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    tmp_f1_micr, tmp_f1_macr, tmp_f1_weight = f1score(label_ids, logits)\n",
        "    # tmp_auc_micr, tmp_auc_macr, tmp_auc_weight = aucscore(label_ids, logits)\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    #calculate f1 scores\n",
        "    f1_micr += tmp_f1_micr\n",
        "    f1_macr += tmp_f1_macr\n",
        "    f1_weight += tmp_f1_weight\n",
        "    print('temp eval acc: ', tmp_eval_accuracy)\n",
        "\n",
        "    #calculate auc\n",
        "    # auc_micr += tmp_auc_micr\n",
        "    # auc_macr += tmp_auc_macr\n",
        "    # auc_weight += tmp_auc_weight\n",
        "\n",
        "    nb_eval_steps += 1\n",
        "  print('TOTAL EVAL LOSS: ', total_eval_loss/len(validation_dataloader))\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "  print('F1 micro: ', f1_micr/nb_eval_steps)\n",
        "  print('F1 macro: ', f1_macr/nb_eval_steps)\n",
        "  print('F1 weighted: ', f1_weight/nb_eval_steps)\n",
        "\n",
        "\n",
        "  # print('AUC macro: ', auc_macr/nb_eval_steps )"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/4 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 1.8007563208474053\n",
            "-----------------------\n",
            "pred:  [7 7 1 1 7 7 2 4 3 4 3 2]\n",
            "labels:  [7 4 1 1 7 2 2 8 3 4 3 4]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [4 1 3 1 0 0 3 2 2 1 4 0]\n",
            "labels:  [7 5 3 4 0 0 3 1 2 4 0 8]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [4 3 0 4 2 1 7 2 0 0 0 0]\n",
            "labels:  [1 3 0 0 1 1 1 2 0 0 5 8]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [2 2 0 0 7 2 0 7 7 7 1 0]\n",
            "labels:  [3 4 1 1 1 4 2 0 7 7 1 0]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [1 2 0 2 2 2 3 0 3 3 0 1]\n",
            "labels:  [1 1 7 1 4 2 1 9 5 3 8 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [1 4 2 0 3 1 3 0 1 7 0 0]\n",
            "labels:  [1 2 2 1 3 3 3 4 1 9 0 4]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 0 1 0 2 2 3 1 7 4 0 0]\n",
            "labels:  [2 5 1 4 1 3 1 7 0 4 1 5]\n",
            "temp eval acc:  0.16666666666666666\n",
            "-----------------------\n",
            "pred:  [1 7 3 0 4 0 1 2 2 0 7 7]\n",
            "labels:  [1 1 3 0 1 2 7 2 2 8 7 7]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [2 3 7 3 3 2 0 3 2 0 2 0]\n",
            "labels:  [7 3 5 2 3 2 9 3 4 0 7 2]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [4 4 4 1 0 4 0 4 2 3 7 2]\n",
            "labels:  [4 8 7 8 2 6 6 0 2 3 1 7]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [3 3 0 0 7 3 1 0 3 7 0 1]\n",
            "labels:  [3 3 4 1 0 3 1 0 7 7 6 1]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [2 7 2 2 7 2 3 0 3 4 0 0]\n",
            "labels:  [2 0 2 1 7 4 3 0 2 8 3 5]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 4 0 2 7 4 1 1 3 2 4 0]\n",
            "labels:  [3 1 4 4 5 1 1 7 3 2 4 4]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [1 2 3 1 3 7 0 2 0 0 0 1]\n",
            "labels:  [1 1 3 3 6 2 1 3 8 0 8 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [7 7 4 3 3 4 0 0 3 0 0 4]\n",
            "labels:  [7 7 5 7 8 4 3 0 3 3 0 4]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [0 3 2 0 0 1 2 3 2 7 0 3]\n",
            "labels:  [5 1 2 3 0 1 7 3 2 7 4 3]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 3 3 0 3 3 7 0 3 2 3 0]\n",
            "labels:  [3 3 8 8 3 5 0 0 2 2 2 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [1 0 3 2 3 0 3 7 3 4 7 0]\n",
            "labels:  [8 2 3 7 8 0 4 7 4 4 4 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [0 7 2 7 3 2 7 7 0 3 0 4]\n",
            "labels:  [3 7 2 7 3 8 7 7 0 3 7 1]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [4 0 0 4 3 2 3 2 3 3 3 7]\n",
            "labels:  [2 1 9 0 1 2 2 1 3 3 3 7]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 0 3 3 1 2 0 7 3 7 2 3]\n",
            "labels:  [3 9 3 3 1 8 2 2 3 0 2 1]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [2 4 0 1 2 4 1 3 4 7 2 4]\n",
            "labels:  [1 1 0 8 1 4 1 3 8 8 2 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 3 0 2 0 4 0 0 4 0 0 1]\n",
            "labels:  [1 3 2 1 2 7 0 0 1 0 1 7]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [2 1 3 0 2 7 7 1 0 1 7 3]\n",
            "labels:  [2 5 3 0 2 7 7 8 0 0 7 3]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [2 3 4 7 0 0 7 0 4 2 1 1]\n",
            "labels:  [8 3 1 5 0 0 7 7 7 2 1 1]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [4 0 2 4 2 1 2 3 0 2 2 7]\n",
            "labels:  [0 9 7 1 4 1 2 3 5 7 1 1]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [4 2 3 3 2 0 2 4 0 3 4 2]\n",
            "labels:  [8 2 3 4 2 1 5 4 1 3 0 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [7 1 2 3 7 4 4 3 2 0 4 3]\n",
            "labels:  [0 8 2 3 8 4 4 3 4 9 4 0]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 3 2 3 0 2 2 3 4 3 7 2]\n",
            "labels:  [3 3 8 4 3 1 1 1 4 3 1 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [4 2 4 7 1 2 1 2 2 2 2 4]\n",
            "labels:  [4 2 1 7 1 1 1 2 2 2 1 0]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [0 7 7 0 3 0 1 3 0 3 2 0]\n",
            "labels:  [0 4 7 1 3 7 8 3 7 3 2 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [2 0 2 1 7 2 7 0 4 3 0 7]\n",
            "labels:  [4 0 2 5 7 2 9 0 4 1 7 4]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 7 3 2 2 3 0 2 0 4 2 1]\n",
            "labels:  [9 7 0 2 1 1 0 2 9 8 7 3]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [1 3 7 0 0 2 7 4 3 1 2 7]\n",
            "labels:  [1 3 7 1 7 2 7 8 3 4 2 1]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 4 0 3 2 1 2 3 3 2 2 3]\n",
            "labels:  [3 0 0 3 0 1 2 1 3 7 4 3]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [0 4 0 2 7 7 0 2 2 3 3 2]\n",
            "labels:  [0 1 1 8 7 7 4 1 2 6 1 7]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [2 2 2 0 3 3 1 0 2 3 4 1]\n",
            "labels:  [5 2 2 8 6 1 1 0 1 3 4 0]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 7 1 7 1 2 4 3 3 4 2 2]\n",
            "labels:  [3 0 1 1 1 2 8 3 2 3 8 2]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [2 0 1 0 7 2 3 0 0 7 0 0]\n",
            "labels:  [2 0 1 1 7 1 8 2 0 7 0 0]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [3 3 2 2 3 1 7 3 4 0 0 0]\n",
            "labels:  [3 3 2 7 3 1 7 1 9 7 8 2]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [2 0 3 3 4 3 7 4 1 0 0 3]\n",
            "labels:  [2 0 2 1 7 3 7 7 7 0 8 3]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 7 3 1 2 3 7 1 4 2 3 4]\n",
            "labels:  [3 4 3 1 2 6 7 0 8 8 1 8]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [1 1 2 4 0 0 0 3 3 4 2 2]\n",
            "labels:  [1 1 2 9 7 4 7 3 3 2 3 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [7 4 0 4 7 2 2 0 0 1 4 0]\n",
            "labels:  [8 1 4 4 1 2 1 0 6 1 7 9]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [7 7 3 2 0 0 4 1 4 2 7 0]\n",
            "labels:  [7 7 3 8 8 0 4 1 7 0 1 5]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 7 7 1 1 3 4 7 2 0 2 2]\n",
            "labels:  [3 4 8 7 1 3 4 0 7 0 2 2]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [0 7 0 1 7 3 7 3 1 0 3 0]\n",
            "labels:  [8 8 4 4 7 3 4 2 1 2 3 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 1 2 0 0 7 2 3 7 1 3 2]\n",
            "labels:  [6 2 1 8 1 7 7 8 4 1 1 1]\n",
            "temp eval acc:  0.16666666666666666\n",
            "-----------------------\n",
            "pred:  [3 2 1 0 2 4 0 2 0 3 2 4]\n",
            "labels:  [5 4 4 4 2 8 0 2 0 3 4 8]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [0 7 0 4 2 0 4 2 3 0 4 2]\n",
            "labels:  [0 0 7 4 7 5 9 2 3 0 1 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [2 4 0 0 7 7 1 3 3 2 1 1]\n",
            "labels:  [4 5 7 4 1 7 4 7 1 2 1 7]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [1 2 1 2 2 0 1 0 3 2 2 0]\n",
            "labels:  [1 5 1 2 8 7 2 8 3 7 4 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [2 3 4 4 4 1 0 0 7 2 0 4]\n",
            "labels:  [2 1 4 4 2 1 1 5 9 1 5 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [4 0 0 2 4 2 4 4 0 2 2 7]\n",
            "labels:  [8 3 0 1 7 2 4 7 0 8 1 7]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [2 3 4 7 2 4 7 7 0 3 2 7]\n",
            "labels:  [4 8 3 1 1 0 2 7 2 3 2 1]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [0 1 3 7 3 3 1 2 0 3 2 7]\n",
            "labels:  [0 1 1 7 1 3 1 2 5 3 2 7]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [0 0 3 0 4 1 2 0 3 4 3 2]\n",
            "labels:  [0 4 3 0 4 4 8 5 3 4 3 2]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [0 3 1 0 1 7 7 0 0 3 1 4]\n",
            "labels:  [0 3 8 2 1 7 0 0 8 3 4 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 2 7 1 3 2 7 2 0 3 4 0]\n",
            "labels:  [4 1 4 1 1 1 7 4 2 3 4 4]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 3 0 7 0 0 7 3 1 7 0 3]\n",
            "labels:  [3 3 1 4 1 9 8 3 4 0 0 3]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 4 0 2 3 4 3 0 0 2 2 3]\n",
            "labels:  [3 0 8 7 2 0 3 2 7 1 2 3]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 1 3 4 2 3 0 3 3 3 3 2]\n",
            "labels:  [3 3 1 0 4 3 0 1 3 0 2 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 1 2 7 4 3 0 2 2 0 2 4]\n",
            "labels:  [3 1 0 7 5 3 0 2 2 2 8 8]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [4 0 3 2 0 1 7 4 0 0 3 1]\n",
            "labels:  [5 0 3 1 1 1 5 4 0 5 3 0]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [4 1 1 0 3 0 0 1 3 3 0 3]\n",
            "labels:  [4 2 1 8 3 2 9 4 1 3 2 7]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [2 7 3 3 4 0 3 0 7 4 3 0]\n",
            "labels:  [2 4 3 3 7 8 1 7 7 4 3 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 7 1 2 4 1 7 7 7 3 4 3]\n",
            "labels:  [3 4 3 2 8 7 7 7 0 3 0 2]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [0 2 0 1 3 4 2 4 4 0 0 0]\n",
            "labels:  [0 7 0 3 2 1 2 0 4 5 5 2]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [2 3 4 0 0 0 3 1 1 3 0 7]\n",
            "labels:  [1 3 2 0 9 1 8 8 4 2 9 0]\n",
            "temp eval acc:  0.16666666666666666\n",
            "-----------------------\n",
            "pred:  [3 0 3 4 3 4 7 3 0 7 3 2]\n",
            "labels:  [1 3 3 5 2 1 7 3 2 7 1 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [0 0 0 4 2 2 0 4 0 0 2 7]\n",
            "labels:  [3 0 6 0 1 2 2 1 3 0 1 1]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [3 1 3 4 4 0 0 1 1 1 0 0]\n",
            "labels:  [1 4 1 8 7 0 4 1 1 0 4 0]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [2 4 4 0 7 7 2 7 7 3 0 2]\n",
            "labels:  [6 7 4 0 7 7 2 4 9 3 2 2]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [2 3 7 2 2 0 1 4 4 0 4 0]\n",
            "labels:  [2 3 7 1 2 0 1 5 4 0 0 2]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [3 2 2 0 0 3 3 1 7 4 0 0]\n",
            "labels:  [1 2 4 2 5 3 3 3 7 1 0 0]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 2 4 0 0 0 1 4 0 0 3 0]\n",
            "labels:  [3 2 0 8 0 9 1 4 0 9 1 0]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [0 3 0 0 4 3 3 0 4 3 7 0]\n",
            "labels:  [0 1 0 1 4 1 3 5 1 3 1 7]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [2 3 3 4 3 1 7 4 3 4 3 1]\n",
            "labels:  [2 3 3 0 3 1 7 7 3 4 1 4]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [0 0 4 7 2 2 1 1 3 0 7 1]\n",
            "labels:  [0 0 4 2 2 4 6 4 3 0 7 4]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [7 7 2 0 0 1 3 0 7 3 1 0]\n",
            "labels:  [2 7 2 0 0 8 6 0 4 0 1 8]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 3 3 1 4 7 4 4 7 0 0 0]\n",
            "labels:  [0 1 3 2 1 1 7 4 7 8 1 7]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [3 4 7 1 7 0 2 2 3 3 2 4]\n",
            "labels:  [3 2 1 3 7 2 2 2 3 1 2 8]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 4 1 3 2 3 3 3 7 7 4 4]\n",
            "labels:  [2 7 1 3 2 3 1 3 7 1 9 3]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [7 3 1 1 4 3 7 7 7 3 0 3]\n",
            "labels:  [8 3 1 1 5 1 7 7 8 3 2 3]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 0 2 4 0 0 3 0 0 1 4 4]\n",
            "labels:  [1 2 2 2 2 5 3 0 3 2 8 4]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 3 0 2 7 3 3 3 1 0 7 4]\n",
            "labels:  [1 3 3 7 4 3 3 1 1 1 7 8]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 0 4 0 0 3 3 1 7 2 1 2]\n",
            "labels:  [6 9 0 0 0 3 3 7 8 1 1 5]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [0 3 2 2 1 2 7 1 3 4 3 7]\n",
            "labels:  [3 3 2 2 8 1 1 8 3 4 3 7]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [0 0 3 4 0 0 0 3 2 1 4 0]\n",
            "labels:  [6 0 2 7 0 0 5 9 2 8 6 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 1 3 7 0 0 0 1 2 0 4 4]\n",
            "labels:  [1 1 2 1 0 0 0 4 2 0 4 1]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [7 3 1 2 0 3 1 3 3 2 4 2]\n",
            "labels:  [4 3 1 7 0 2 7 3 0 2 7 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 4 2 7 7 4 1 3 2 3 3 3]\n",
            "labels:  [5 1 7 7 0 1 1 3 1 3 1 3]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [1 7 3 0 3 2 4 0 0 3 3 7]\n",
            "labels:  [2 7 3 9 3 1 1 0 0 1 3 7]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [7 2 4 3 3 1 2 1 3 1 1 0]\n",
            "labels:  [4 1 4 1 3 1 7 4 2 4 3 0]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 3 7 0 1 2 4 0 2 0 0 2]\n",
            "labels:  [1 3 1 5 7 2 4 8 1 2 0 4]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 4 4 0 4 2 1 7 0 1 2 0]\n",
            "labels:  [3 7 4 2 3 2 3 7 0 1 7 0]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 3 7 2 4 4 1 3 3 0 3 7]\n",
            "labels:  [3 3 8 1 6 4 9 3 7 0 1 4]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [0 7 0 1 2 0 0 2 3 1 3 1]\n",
            "labels:  [0 7 0 9 1 0 8 3 3 8 1 8]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [0 3 4 1 7 1 0 0 4 2 3 2]\n",
            "labels:  [1 3 0 4 7 1 5 0 4 4 3 2]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 2 0 0 3 0 2 2 3 2 4 4]\n",
            "labels:  [0 2 0 0 3 7 4 2 3 1 4 4]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [3 3 3 3 2 0 3 2 4 7 2 3]\n",
            "labels:  [3 3 4 3 2 0 3 2 7 7 4 1]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [4 3 2 3 3 3 2 2 0 2 3 3]\n",
            "labels:  [4 3 1 3 3 3 2 2 0 1 3 3]\n",
            "temp eval acc:  0.8333333333333334\n",
            "-----------------------\n",
            "pred:  [3 2 4 7 3 2 1 4 7 1 4 0]\n",
            "labels:  [5 2 8 7 2 1 1 5 7 3 1 3]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [0 3 7 7 3 2 2 3 3 7 4 7]\n",
            "labels:  [0 3 1 1 3 2 1 3 7 4 7 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [4 2 2 7 7 3 2 3 4 0 3 1]\n",
            "labels:  [8 2 4 7 7 5 2 3 4 5 3 2]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 1 0 0 1 1 2 1 0 2 2 7]\n",
            "labels:  [1 1 7 8 1 1 5 5 1 2 8 8]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [4 0 0 1 0 3 1 2 4 4 4 0]\n",
            "labels:  [1 3 0 1 0 3 1 2 9 4 4 8]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [7 2 0 3 0 7 3 0 3 0 2 7]\n",
            "labels:  [7 2 9 3 0 1 8 0 1 4 2 7]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [2 0 3 3 0 7 3 3 1 3 0 1]\n",
            "labels:  [1 2 0 1 8 7 3 3 4 3 0 4]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [0 3 3 7 4 4 2 0 0 3 1 4]\n",
            "labels:  [5 3 3 7 0 1 2 0 7 3 3 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 3 4 7 3 2 0 0 2 3 4 7]\n",
            "labels:  [4 1 1 4 3 2 9 2 2 1 4 7]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [2 2 0 3 2 7 0 3 7 4 3 7]\n",
            "labels:  [2 1 8 7 4 1 3 1 1 7 5 4]\n",
            "temp eval acc:  0.08333333333333333\n",
            "-----------------------\n",
            "pred:  [0 0 4 3 7 3 4 7 2 3 2 0]\n",
            "labels:  [5 0 9 3 7 3 8 2 2 0 1 2]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [7 1 2 4 1 0 0 4 3 1 0 0]\n",
            "labels:  [7 4 2 8 7 7 8 5 1 1 4 7]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [7 0 0 4 7 7 2 3 7 7 1 0]\n",
            "labels:  [7 0 1 1 1 8 2 3 4 4 1 7]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [7 3 7 2 3 3 7 4 7 0 4 2]\n",
            "labels:  [7 3 7 1 0 3 1 4 7 6 3 2]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 3 2 0 0 7 7 4 2 3 3 0]\n",
            "labels:  [1 1 3 1 3 7 1 4 1 3 0 7]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [2 7 4 0 2 4 7 1 7 3 2 0]\n",
            "labels:  [1 7 7 0 2 4 4 1 8 2 2 0]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [7 4 0 4 0 3 2 3 4 2 7 2]\n",
            "labels:  [8 4 4 7 0 4 2 1 7 3 4 4]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [2 3 2 4 7 3 7 7 2 7 7 4]\n",
            "labels:  [2 3 4 5 7 3 7 7 1 7 7 4]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [7 2 4 7 4 7 0 3 2 0 0 0]\n",
            "labels:  [7 2 4 1 1 7 0 3 2 1 5 0]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [4 3 4 0 2 2 7 2 7 3 4 4]\n",
            "labels:  [4 1 4 1 1 1 7 2 7 3 4 4]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [0 0 3 2 1 2 0 4 4 0 2 3]\n",
            "labels:  [8 2 3 1 4 8 0 1 4 7 1 0]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [0 2 4 1 7 3 3 0 7 7 2 0]\n",
            "labels:  [0 2 4 2 2 3 1 0 7 7 2 2]\n",
            "temp eval acc:  0.6666666666666666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  25%|██▌       | 1/4 [14:55<44:45, 895.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------------------\n",
            "pred:  [3 2 1 7 1 3 1 0 0 4 2 0]\n",
            "labels:  [2 2 1 7 4 2 1 0 5 7 2 0]\n",
            "temp eval acc:  0.5833333333333334\n",
            "TOTAL EVAL LOSS:  1.5789905786514282\n",
            "Validation Accuracy: 0.46333333333333326\n",
            "F1 micro:  0.46333333333333326\n",
            "F1 macro:  0.34469988524702816\n",
            "F1 weighted:  0.42997623857623835\n",
            "Train loss: 1.5402531736691794\n",
            "-----------------------\n",
            "pred:  [7 7 1 1 7 7 2 7 3 7 3 2]\n",
            "labels:  [7 4 1 1 7 2 2 8 3 4 3 4]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [4 1 3 7 2 0 3 2 2 4 4 9]\n",
            "labels:  [7 5 3 4 0 0 3 1 2 4 0 8]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [4 3 0 4 2 1 7 2 0 0 0 8]\n",
            "labels:  [1 3 0 0 1 1 1 2 0 0 5 8]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [2 2 0 0 7 1 9 4 7 7 1 9]\n",
            "labels:  [3 4 1 1 1 4 2 0 7 7 1 0]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [1 2 4 2 2 2 3 0 1 3 0 1]\n",
            "labels:  [1 1 7 1 4 2 1 9 5 3 8 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [1 1 2 1 4 1 3 2 1 7 0 4]\n",
            "labels:  [1 2 2 1 3 3 3 4 1 9 0 4]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 0 1 0 1 1 3 4 7 7 1 0]\n",
            "labels:  [2 5 1 4 1 3 1 7 0 4 1 5]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [1 7 3 1 4 0 7 7 2 0 7 7]\n",
            "labels:  [1 1 3 0 1 2 7 2 2 8 7 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [7 3 7 2 3 1 2 3 7 0 2 0]\n",
            "labels:  [7 3 5 2 3 2 9 3 4 0 7 2]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [4 4 4 4 3 7 0 4 1 3 7 7]\n",
            "labels:  [4 8 7 8 2 6 6 0 2 3 1 7]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [3 3 0 1 7 3 7 0 3 7 7 1]\n",
            "labels:  [3 3 4 1 0 3 1 0 7 7 6 1]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [2 7 2 2 7 1 3 7 3 4 0 0]\n",
            "labels:  [2 0 2 1 7 4 3 0 2 8 3 5]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 1 4 7 7 4 1 7 3 1 4 0]\n",
            "labels:  [3 1 4 4 5 1 1 7 3 2 4 4]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 2 3 7 3 7 3 1 0 0 0 1]\n",
            "labels:  [1 1 3 3 6 2 1 3 8 0 8 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [7 7 0 1 3 4 3 0 3 3 8 4]\n",
            "labels:  [7 7 5 7 8 4 3 0 3 3 0 4]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [1 3 2 1 0 7 2 3 2 7 4 3]\n",
            "labels:  [5 1 2 3 0 1 7 3 2 7 4 3]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 3 3 0 3 3 7 0 1 2 3 1]\n",
            "labels:  [3 3 8 8 3 5 0 0 2 2 2 1]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 2 3 7 3 3 1 7 3 4 7 4]\n",
            "labels:  [8 2 3 7 8 0 4 7 4 4 4 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [0 7 2 7 3 7 7 7 0 1 0 1]\n",
            "labels:  [3 7 2 7 3 8 7 7 0 3 7 1]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [4 7 9 4 1 2 3 4 3 3 3 7]\n",
            "labels:  [2 1 9 0 1 2 2 1 3 3 3 7]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 0 3 3 1 2 1 7 3 7 2 3]\n",
            "labels:  [3 9 3 3 1 8 2 2 3 0 2 1]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 7 0 7 7 4 1 3 4 7 2 4]\n",
            "labels:  [1 1 0 8 1 4 1 3 8 8 2 1]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 3 7 1 0 4 4 1 4 7 0 7]\n",
            "labels:  [1 3 2 1 2 7 0 0 1 0 1 7]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [1 1 3 1 2 7 7 1 0 7 7 3]\n",
            "labels:  [2 5 3 0 2 7 7 8 0 0 7 3]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [7 3 4 7 7 1 7 0 7 2 1 7]\n",
            "labels:  [8 3 1 5 0 0 7 7 7 2 1 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [1 9 7 4 1 7 7 3 0 7 2 7]\n",
            "labels:  [0 9 7 1 4 1 2 3 5 7 1 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [4 2 3 4 2 4 1 4 1 3 4 1]\n",
            "labels:  [8 2 3 4 2 1 5 4 1 3 0 1]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [7 1 7 3 7 7 4 3 2 7 4 3]\n",
            "labels:  [0 8 2 3 8 4 4 3 4 9 4 0]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 3 2 1 1 7 2 1 1 1 7 2]\n",
            "labels:  [3 3 8 4 3 1 1 1 4 3 1 1]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [4 2 7 7 1 7 1 2 2 2 7 7]\n",
            "labels:  [4 2 1 7 1 1 1 2 2 2 1 0]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [0 7 7 1 3 7 1 3 7 3 2 0]\n",
            "labels:  [0 4 7 1 3 7 8 3 7 3 2 7]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [2 3 2 1 7 7 7 1 4 1 1 7]\n",
            "labels:  [4 0 2 5 7 2 9 0 4 1 7 4]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 7 3 2 1 1 9 2 0 4 7 1]\n",
            "labels:  [9 7 0 2 1 1 0 2 9 8 7 3]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 3 7 0 0 2 7 7 3 7 2 7]\n",
            "labels:  [1 3 7 1 7 2 7 8 3 4 2 1]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 4 0 3 2 1 2 1 3 7 1 3]\n",
            "labels:  [3 0 0 3 0 1 2 1 3 7 4 3]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [0 4 1 2 7 7 0 7 1 3 1 2]\n",
            "labels:  [0 1 1 8 7 7 4 1 2 6 1 7]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [2 2 1 9 3 3 1 0 2 3 4 1]\n",
            "labels:  [5 2 2 8 6 1 1 0 1 3 4 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 7 1 7 1 1 4 3 3 4 2 7]\n",
            "labels:  [3 0 1 1 1 2 8 3 2 3 8 2]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [2 8 1 2 7 7 3 2 0 7 0 0]\n",
            "labels:  [2 0 1 1 7 1 8 2 0 7 0 0]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [3 3 2 7 3 1 7 1 4 0 0 2]\n",
            "labels:  [3 3 2 7 3 1 7 1 9 7 8 2]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [2 0 1 4 1 3 7 4 7 0 0 3]\n",
            "labels:  [2 0 2 1 7 3 7 7 7 0 8 3]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 7 3 1 2 3 7 1 7 1 1 4]\n",
            "labels:  [3 4 3 1 2 6 7 0 8 8 1 8]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 1 1 4 0 4 0 3 3 4 1 2]\n",
            "labels:  [1 1 2 9 7 4 7 3 3 2 3 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [7 4 0 4 7 2 2 0 0 1 4 0]\n",
            "labels:  [8 1 4 4 1 2 1 0 6 1 7 9]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [7 7 3 2 1 4 4 1 7 2 7 0]\n",
            "labels:  [7 7 3 8 8 0 4 1 7 0 1 5]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 7 7 7 4 3 4 7 2 0 2 7]\n",
            "labels:  [3 4 8 7 1 3 4 0 7 0 2 2]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [0 7 0 1 7 3 1 1 1 2 3 0]\n",
            "labels:  [8 8 4 4 7 3 4 2 1 2 3 0]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 7 7 0 1 7 1 1 7 1 3 1]\n",
            "labels:  [6 2 1 8 1 7 7 8 4 1 1 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [4 1 1 0 2 7 3 2 0 3 4 4]\n",
            "labels:  [5 4 4 4 2 8 0 2 0 3 4 8]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [7 7 0 4 2 1 4 2 3 0 4 1]\n",
            "labels:  [0 0 7 4 7 5 9 2 3 0 1 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [4 4 0 4 7 7 4 4 1 2 1 7]\n",
            "labels:  [4 5 7 4 1 7 4 7 1 2 1 7]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [7 2 1 2 7 4 1 0 3 7 2 0]\n",
            "labels:  [1 5 1 2 8 7 2 8 3 7 4 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [2 3 4 4 4 1 8 0 7 2 0 4]\n",
            "labels:  [2 1 4 4 2 1 1 5 9 1 5 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [4 3 7 2 7 2 4 4 0 2 7 7]\n",
            "labels:  [8 3 0 1 7 2 4 7 0 8 1 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [2 3 4 7 2 7 7 7 0 3 7 7]\n",
            "labels:  [4 8 3 1 1 0 2 7 2 3 2 1]\n",
            "temp eval acc:  0.16666666666666666\n",
            "-----------------------\n",
            "pred:  [0 1 3 7 1 3 7 2 0 3 2 7]\n",
            "labels:  [0 1 1 7 1 3 1 2 5 3 2 7]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [2 4 3 0 0 4 2 0 3 4 3 2]\n",
            "labels:  [0 4 3 0 4 4 8 5 3 4 3 2]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [0 3 1 0 7 7 7 1 0 3 1 4]\n",
            "labels:  [0 3 8 2 1 7 0 0 8 3 4 7]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [1 7 7 1 3 2 7 4 0 1 4 0]\n",
            "labels:  [4 1 4 1 1 1 7 4 2 3 4 4]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 3 0 7 0 0 7 3 1 7 7 3]\n",
            "labels:  [3 3 1 4 1 9 8 3 4 0 0 3]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 0 0 7 3 4 3 2 0 7 7 3]\n",
            "labels:  [3 0 8 7 2 0 3 2 7 1 2 3]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 1 1 7 2 3 7 4 3 3 3 7]\n",
            "labels:  [3 3 1 0 4 3 0 1 3 0 2 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 1 4 7 4 3 0 2 2 7 2 8]\n",
            "labels:  [3 1 0 7 5 3 0 2 2 2 8 8]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [7 0 3 7 1 1 7 4 0 0 3 1]\n",
            "labels:  [5 0 3 1 1 1 5 4 0 5 3 0]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [4 1 1 0 3 7 2 1 1 3 2 3]\n",
            "labels:  [4 2 1 8 3 2 9 4 1 3 2 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [2 7 3 3 7 0 3 0 7 4 3 0]\n",
            "labels:  [2 4 3 3 7 8 1 7 7 4 3 7]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 7 4 1 7 1 7 7 7 3 4 3]\n",
            "labels:  [3 4 3 2 8 7 7 7 0 3 0 2]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [0 2 0 1 3 7 7 0 4 4 0 0]\n",
            "labels:  [0 7 0 3 2 1 2 0 4 5 5 2]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [7 3 7 0 4 1 1 7 1 3 9 7]\n",
            "labels:  [1 3 2 0 9 1 8 8 4 2 9 0]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [1 1 1 4 3 7 7 3 0 7 3 7]\n",
            "labels:  [1 3 3 5 2 1 7 3 2 7 1 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [2 0 0 4 1 2 0 4 3 3 7 1]\n",
            "labels:  [3 0 6 0 1 2 2 1 3 0 1 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 1 1 4 7 1 0 1 7 1 0 4]\n",
            "labels:  [1 4 1 8 7 0 4 1 1 0 4 0]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [2 7 7 0 7 7 2 4 7 3 2 2]\n",
            "labels:  [6 7 4 0 7 7 2 4 9 3 2 2]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [7 3 7 1 2 0 1 7 4 0 1 0]\n",
            "labels:  [2 3 7 1 2 0 1 5 4 0 0 2]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [3 2 2 0 4 1 3 1 7 4 0 2]\n",
            "labels:  [1 2 4 2 5 3 3 3 7 1 0 0]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 2 4 0 0 9 1 4 0 0 3 0]\n",
            "labels:  [3 2 0 8 0 9 1 4 0 9 1 0]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [0 1 0 3 4 3 3 0 7 3 7 0]\n",
            "labels:  [0 1 0 1 4 1 3 5 1 3 1 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [2 3 3 4 3 1 7 4 1 4 1 1]\n",
            "labels:  [2 3 3 0 3 1 7 7 3 4 1 4]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [7 0 1 7 2 1 4 1 3 2 7 1]\n",
            "labels:  [0 0 4 2 2 4 6 4 3 0 7 4]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [7 7 2 3 0 4 1 0 7 3 1 0]\n",
            "labels:  [2 7 2 0 0 8 6 0 4 0 1 8]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 3 3 1 4 7 4 4 7 9 0 9]\n",
            "labels:  [0 1 3 2 1 1 7 4 7 8 1 7]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [3 7 7 7 7 7 2 2 3 1 2 7]\n",
            "labels:  [3 2 1 3 7 2 2 2 3 1 2 8]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 4 1 1 7 3 3 3 7 7 7 4]\n",
            "labels:  [2 7 1 3 2 3 1 3 7 1 9 3]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [7 3 1 1 4 1 7 7 7 3 0 3]\n",
            "labels:  [8 3 1 1 5 1 7 7 8 3 2 3]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [1 0 2 4 0 7 1 0 3 1 7 4]\n",
            "labels:  [1 2 2 2 2 5 3 0 3 2 8 4]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 3 1 7 7 3 3 3 1 1 7 4]\n",
            "labels:  [1 3 3 7 4 3 3 1 1 1 7 8]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 1 4 1 0 3 3 7 7 2 1 7]\n",
            "labels:  [6 9 0 0 0 3 3 7 8 1 1 5]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [4 3 2 2 1 7 7 1 3 4 3 7]\n",
            "labels:  [3 3 2 2 8 1 1 8 3 4 3 7]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [0 0 3 4 0 0 0 1 2 7 7 0]\n",
            "labels:  [6 0 2 7 0 0 5 9 2 8 6 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [1 1 1 7 0 0 3 1 7 0 7 1]\n",
            "labels:  [1 1 2 1 0 0 0 4 2 0 4 1]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [7 3 1 2 7 3 4 3 3 2 7 7]\n",
            "labels:  [4 3 1 7 0 2 7 3 0 2 7 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [1 7 7 7 7 7 1 3 2 3 3 3]\n",
            "labels:  [5 1 7 7 0 1 1 3 1 3 1 3]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 7 3 7 3 1 4 7 0 1 3 7]\n",
            "labels:  [2 7 3 9 3 1 1 0 0 1 3 7]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [7 1 7 3 3 1 7 1 1 1 1 0]\n",
            "labels:  [4 1 4 1 3 1 7 4 2 4 3 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 3 7 2 1 1 4 0 1 1 0 2]\n",
            "labels:  [1 3 1 5 7 2 4 8 1 2 0 4]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 7 4 9 4 2 1 7 0 4 2 0]\n",
            "labels:  [3 7 4 2 3 2 3 7 0 1 7 0]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 3 7 2 4 4 1 3 1 0 3 7]\n",
            "labels:  [3 3 8 1 6 4 9 3 7 0 1 4]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [0 1 0 4 2 2 4 1 3 4 3 1]\n",
            "labels:  [0 7 0 9 1 0 8 3 3 8 1 8]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [1 3 4 1 7 1 0 0 4 1 3 2]\n",
            "labels:  [1 3 0 4 7 1 5 0 4 4 3 2]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [1 2 1 1 3 0 1 2 1 1 4 7]\n",
            "labels:  [0 2 0 0 3 7 4 2 3 1 4 4]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 3 1 3 2 0 3 2 1 7 2 3]\n",
            "labels:  [3 3 4 3 2 0 3 2 7 7 4 1]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [7 3 2 3 3 3 7 2 2 7 3 3]\n",
            "labels:  [4 3 1 3 3 3 2 2 0 1 3 3]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 1 4 7 1 2 1 4 7 1 4 1]\n",
            "labels:  [5 2 8 7 2 1 1 5 7 3 1 3]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [4 1 7 7 3 2 2 3 3 7 4 7]\n",
            "labels:  [0 3 1 1 3 2 1 3 7 4 7 1]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [4 2 2 7 7 3 2 3 4 7 3 7]\n",
            "labels:  [8 2 4 7 7 5 2 3 4 5 3 2]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 1 0 0 1 1 2 4 0 2 2 7]\n",
            "labels:  [1 1 7 8 1 1 5 5 1 2 8 8]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [8 3 0 1 0 3 1 2 7 4 1 2]\n",
            "labels:  [1 3 0 1 0 3 1 2 9 4 4 8]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [7 2 0 3 0 7 1 3 3 0 2 7]\n",
            "labels:  [7 2 9 3 0 1 8 0 1 4 2 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [2 0 3 3 0 7 3 3 1 3 7 1]\n",
            "labels:  [1 2 0 1 8 7 3 3 4 3 0 4]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [7 1 3 7 4 4 2 0 0 3 1 7]\n",
            "labels:  [5 3 3 7 0 1 2 0 7 3 3 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 3 7 7 3 2 0 2 7 1 4 7]\n",
            "labels:  [4 1 1 4 3 2 9 2 2 1 4 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [2 7 1 7 2 7 3 3 7 7 3 7]\n",
            "labels:  [2 1 8 7 4 1 3 1 1 7 5 4]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [0 0 1 3 7 3 7 7 2 3 2 4]\n",
            "labels:  [5 0 9 3 7 3 8 2 2 0 1 2]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [7 1 2 4 1 0 4 1 3 4 7 7]\n",
            "labels:  [7 4 2 8 7 7 8 5 1 1 4 7]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [7 4 0 4 7 7 2 3 7 7 1 7]\n",
            "labels:  [7 0 1 1 1 8 2 3 4 4 1 7]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [7 3 7 2 3 3 7 4 7 9 1 2]\n",
            "labels:  [7 3 7 1 0 3 1 4 7 6 3 2]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 1 1 9 1 7 7 4 1 3 3 3]\n",
            "labels:  [1 1 3 1 3 7 1 4 1 3 0 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [7 7 7 0 7 4 7 1 7 3 2 0]\n",
            "labels:  [1 7 7 0 2 4 4 1 8 2 2 0]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [7 4 0 4 0 3 2 3 4 2 7 1]\n",
            "labels:  [8 4 4 7 0 4 2 1 7 3 4 4]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [2 3 2 4 7 3 7 7 2 7 7 4]\n",
            "labels:  [2 3 4 5 7 3 7 7 1 7 7 4]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [7 2 7 7 7 7 0 3 2 1 0 1]\n",
            "labels:  [7 2 4 1 1 7 0 3 2 1 5 0]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [4 1 4 0 2 2 7 2 7 3 4 4]\n",
            "labels:  [4 1 4 1 1 1 7 2 7 3 4 4]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [0 0 3 1 7 2 0 4 4 0 1 4]\n",
            "labels:  [8 2 3 1 4 8 0 1 4 7 1 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [9 2 4 1 7 3 1 9 7 7 2 7]\n",
            "labels:  [0 2 4 2 2 3 1 0 7 7 2 2]\n",
            "temp eval acc:  0.5833333333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 2/4 [29:51<29:50, 895.45s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------------------\n",
            "pred:  [3 2 1 7 1 3 1 0 7 7 2 9]\n",
            "labels:  [2 2 1 7 4 2 1 0 5 7 2 0]\n",
            "temp eval acc:  0.5833333333333334\n",
            "TOTAL EVAL LOSS:  1.6394596099853516\n",
            "Validation Accuracy: 0.4766666666666666\n",
            "F1 micro:  0.4766666666666666\n",
            "F1 macro:  0.35897417233560075\n",
            "F1 weighted:  0.4521825396825398\n",
            "Train loss: 1.4231456512080298\n",
            "-----------------------\n",
            "pred:  [7 1 1 1 7 7 2 4 3 7 3 1]\n",
            "labels:  [7 4 1 1 7 2 2 8 3 4 3 4]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [4 1 3 4 2 0 3 2 2 4 4 0]\n",
            "labels:  [7 5 3 4 0 0 3 1 2 4 0 8]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 3 0 4 1 1 0 2 4 0 0 8]\n",
            "labels:  [1 3 0 0 1 1 1 2 0 0 5 8]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [2 4 2 0 7 1 7 4 7 7 1 0]\n",
            "labels:  [3 4 1 1 1 4 2 0 7 7 1 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [1 2 4 2 1 2 3 3 3 3 8 1]\n",
            "labels:  [1 1 7 1 4 2 1 9 5 3 8 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [1 1 2 1 4 1 3 0 1 1 0 4]\n",
            "labels:  [1 2 2 1 3 3 3 4 1 9 0 4]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 0 1 0 1 3 3 4 0 7 1 0]\n",
            "labels:  [2 5 1 4 1 3 1 7 0 4 1 5]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [1 7 3 1 1 8 1 2 2 0 1 7]\n",
            "labels:  [1 1 3 0 1 2 7 2 2 8 7 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 3 7 3 3 1 0 3 4 0 2 0]\n",
            "labels:  [7 3 5 2 3 2 9 3 4 0 7 2]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [4 4 1 1 3 0 0 4 1 3 7 1]\n",
            "labels:  [4 8 7 8 2 6 6 0 2 3 1 7]\n",
            "temp eval acc:  0.16666666666666666\n",
            "-----------------------\n",
            "pred:  [3 3 0 1 7 3 1 0 3 7 7 1]\n",
            "labels:  [3 3 4 1 0 3 1 0 7 7 6 1]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [2 7 2 2 7 1 3 1 3 4 4 0]\n",
            "labels:  [2 0 2 1 7 4 3 0 2 8 3 5]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 1 4 1 7 4 1 7 3 1 4 0]\n",
            "labels:  [3 1 4 4 5 1 1 7 3 2 4 4]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 2 3 1 3 1 0 3 0 0 0 1]\n",
            "labels:  [1 1 3 3 6 2 1 3 8 0 8 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [7 7 0 1 3 4 0 0 3 3 8 4]\n",
            "labels:  [7 7 5 7 8 4 3 0 3 3 0 4]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [0 3 2 3 0 1 2 3 2 7 4 3]\n",
            "labels:  [5 1 2 3 0 1 7 3 2 7 4 3]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [3 3 3 8 3 3 7 0 1 2 3 1]\n",
            "labels:  [3 3 8 8 3 5 0 0 2 2 2 1]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 1 3 7 3 0 1 7 3 4 1 4]\n",
            "labels:  [8 2 3 7 8 0 4 7 4 4 4 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [0 7 2 7 3 1 7 7 0 3 0 1]\n",
            "labels:  [3 7 2 7 3 8 7 7 0 3 7 1]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [4 7 0 4 1 2 3 1 3 3 3 7]\n",
            "labels:  [2 1 9 0 1 2 2 1 3 3 3 7]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 0 3 3 1 2 3 7 3 7 2 3]\n",
            "labels:  [3 9 3 3 1 8 2 2 3 0 2 1]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 1 0 1 7 4 1 3 4 7 1 4]\n",
            "labels:  [1 1 0 8 1 4 1 3 8 8 2 1]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 3 0 3 0 4 4 3 4 0 0 7]\n",
            "labels:  [1 3 2 1 2 7 0 0 1 0 1 7]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [1 1 3 1 2 7 7 1 0 1 7 3]\n",
            "labels:  [2 5 3 0 2 7 7 8 0 0 7 3]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 3 1 7 7 3 7 0 7 1 1 1]\n",
            "labels:  [8 3 1 5 0 0 7 7 7 2 1 1]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [4 9 1 4 1 1 7 3 0 2 2 7]\n",
            "labels:  [0 9 7 1 4 1 2 3 5 7 1 1]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [4 2 3 4 2 4 1 4 1 3 8 1]\n",
            "labels:  [8 2 3 4 2 1 5 4 1 3 0 1]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [0 1 2 3 7 4 4 3 1 9 4 0]\n",
            "labels:  [0 8 2 3 8 4 4 3 4 9 4 0]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [9 3 1 1 1 1 1 3 4 3 7 1]\n",
            "labels:  [3 3 8 4 3 1 1 1 4 3 1 1]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [4 2 1 7 1 4 1 2 2 8 1 7]\n",
            "labels:  [4 2 1 7 1 1 1 2 2 2 1 0]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [0 7 4 1 3 0 3 3 0 3 2 0]\n",
            "labels:  [0 4 7 1 3 7 8 3 7 3 2 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 0 2 1 1 7 7 0 4 1 3 7]\n",
            "labels:  [4 0 2 5 7 2 9 0 4 1 7 4]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [0 8 3 2 1 3 4 2 0 4 4 1]\n",
            "labels:  [9 7 0 2 1 1 0 2 9 8 7 3]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [1 3 1 0 0 2 7 8 3 1 1 7]\n",
            "labels:  [1 3 7 1 7 2 7 8 3 4 2 1]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [0 0 0 3 2 1 2 1 3 1 1 3]\n",
            "labels:  [3 0 0 3 0 1 2 1 3 7 4 3]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [0 1 1 1 7 7 0 7 1 3 3 4]\n",
            "labels:  [0 1 1 8 7 7 4 1 2 6 1 7]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [2 2 1 9 3 3 1 0 2 3 4 1]\n",
            "labels:  [5 2 2 8 6 1 1 0 1 3 4 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 7 1 7 1 1 4 3 3 4 1 2]\n",
            "labels:  [3 0 1 1 1 2 8 3 2 3 8 2]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [2 0 1 2 7 1 3 0 0 7 0 1]\n",
            "labels:  [2 0 1 1 7 1 8 2 0 7 0 0]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [3 3 2 1 3 1 7 3 4 0 0 0]\n",
            "labels:  [3 3 2 7 3 1 7 1 9 7 8 2]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [2 0 1 4 1 3 7 4 1 0 0 3]\n",
            "labels:  [2 0 2 1 7 3 7 7 7 0 8 3]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 7 3 1 2 3 7 1 1 1 3 4]\n",
            "labels:  [3 4 3 1 2 6 7 0 8 8 1 8]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [1 1 1 4 0 4 0 3 3 4 1 0]\n",
            "labels:  [1 1 2 9 7 4 7 3 3 2 3 0]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [8 1 0 4 7 2 2 0 0 1 4 0]\n",
            "labels:  [8 1 4 4 1 2 1 0 6 1 7 9]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [7 7 3 2 1 4 7 1 4 0 7 0]\n",
            "labels:  [7 7 3 8 8 0 4 1 7 0 1 5]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 7 7 7 4 3 4 7 2 0 1 4]\n",
            "labels:  [3 4 8 7 1 3 4 0 7 0 2 2]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [0 7 0 1 7 3 1 1 1 0 3 0]\n",
            "labels:  [8 8 4 4 7 3 4 2 1 2 3 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 7 7 0 1 7 1 1 7 1 3 1]\n",
            "labels:  [6 2 1 8 1 7 7 8 4 1 1 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [1 1 1 0 2 8 0 2 0 3 1 4]\n",
            "labels:  [5 4 4 4 2 8 0 2 0 3 4 8]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [7 0 0 4 2 1 4 2 3 0 4 3]\n",
            "labels:  [0 0 7 4 7 5 9 2 3 0 1 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [1 4 8 4 7 7 4 4 1 2 1 1]\n",
            "labels:  [4 5 7 4 1 7 4 7 1 2 1 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 1 1 1 1 4 1 0 3 2 2 0]\n",
            "labels:  [1 5 1 2 8 7 2 8 3 7 4 0]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [2 3 4 4 1 1 8 0 0 2 0 4]\n",
            "labels:  [2 1 4 4 2 1 1 5 9 1 5 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [4 3 0 2 7 2 4 4 0 2 7 7]\n",
            "labels:  [8 3 0 1 7 2 4 7 0 8 1 7]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 3 4 7 3 7 7 7 0 3 1 7]\n",
            "labels:  [4 8 3 1 1 0 2 7 2 3 2 1]\n",
            "temp eval acc:  0.16666666666666666\n",
            "-----------------------\n",
            "pred:  [0 1 3 7 1 3 1 2 0 3 2 7]\n",
            "labels:  [0 1 1 7 1 3 1 2 5 3 2 7]\n",
            "temp eval acc:  0.8333333333333334\n",
            "-----------------------\n",
            "pred:  [0 4 3 0 1 8 0 0 3 4 3 1]\n",
            "labels:  [0 4 3 0 4 4 8 5 3 4 3 2]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [0 3 1 0 1 7 7 1 0 3 1 4]\n",
            "labels:  [0 3 8 2 1 7 0 0 8 3 4 7]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [1 7 7 1 3 2 1 4 0 3 4 0]\n",
            "labels:  [4 1 4 1 1 1 7 4 2 3 4 4]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 3 0 7 0 1 7 3 1 7 0 3]\n",
            "labels:  [3 3 1 4 1 9 8 3 4 0 0 3]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 0 0 7 3 4 3 2 0 1 2 3]\n",
            "labels:  [3 0 8 7 2 0 3 2 7 1 2 3]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [3 1 1 0 2 3 0 4 3 1 3 1]\n",
            "labels:  [3 3 1 0 4 3 0 1 3 0 2 1]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 1 0 7 4 3 0 2 1 9 2 8]\n",
            "labels:  [3 1 0 7 5 3 0 2 2 2 8 8]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [4 0 3 1 1 1 8 4 0 0 3 1]\n",
            "labels:  [5 0 3 1 1 1 5 4 0 5 3 0]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [4 1 1 0 3 0 9 1 3 3 0 3]\n",
            "labels:  [4 2 1 8 3 2 9 4 1 3 2 7]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [2 7 3 3 1 0 3 0 7 4 3 0]\n",
            "labels:  [2 4 3 3 7 8 1 7 7 4 3 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 7 1 1 7 1 0 7 7 3 4 3]\n",
            "labels:  [3 4 3 2 8 7 7 7 0 3 0 2]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [0 2 0 1 3 7 7 4 4 4 0 3]\n",
            "labels:  [0 7 0 3 2 1 2 0 4 5 5 2]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [7 3 7 0 0 0 3 1 1 3 9 7]\n",
            "labels:  [1 3 2 0 9 1 8 8 4 2 9 0]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [1 1 3 4 3 1 7 3 0 7 3 7]\n",
            "labels:  [1 3 3 5 2 1 7 3 2 7 1 1]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [0 0 0 4 1 2 0 4 3 3 4 1]\n",
            "labels:  [3 0 6 0 1 2 2 1 3 0 1 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 1 1 8 4 1 0 1 1 1 0 4]\n",
            "labels:  [1 4 1 8 7 0 4 1 1 0 4 0]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [2 8 4 0 7 7 3 4 7 3 2 2]\n",
            "labels:  [6 7 4 0 7 7 2 4 9 3 2 2]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [4 3 7 1 1 0 1 8 4 0 1 0]\n",
            "labels:  [2 3 7 1 2 0 1 5 4 0 0 2]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 2 4 0 4 1 3 1 7 4 0 8]\n",
            "labels:  [1 2 4 2 5 3 3 3 7 1 0 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 2 4 0 0 0 1 4 8 0 3 0]\n",
            "labels:  [3 2 0 8 0 9 1 4 0 9 1 0]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [0 1 0 1 4 3 3 0 1 3 7 0]\n",
            "labels:  [0 1 0 1 4 1 3 5 1 3 1 7]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [2 3 3 4 3 1 7 8 3 4 1 1]\n",
            "labels:  [2 3 3 0 3 1 7 7 3 4 1 4]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [0 0 1 7 2 1 4 1 3 1 7 1]\n",
            "labels:  [0 0 4 2 2 4 6 4 3 0 7 4]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [7 7 2 0 0 1 3 0 7 3 1 0]\n",
            "labels:  [2 7 2 0 0 8 6 0 4 0 1 8]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 3 3 1 4 1 4 4 7 9 0 7]\n",
            "labels:  [0 1 3 2 1 1 7 4 7 8 1 7]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 7 7 1 7 7 2 2 3 3 4 1]\n",
            "labels:  [3 2 1 3 7 2 2 2 3 1 2 8]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 4 1 3 7 3 3 3 7 7 4 4]\n",
            "labels:  [2 7 1 3 2 3 1 3 7 1 9 3]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [7 3 1 1 4 1 7 7 7 3 0 3]\n",
            "labels:  [8 3 1 1 5 1 7 7 8 3 2 3]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [1 8 2 4 0 8 1 0 0 1 4 4]\n",
            "labels:  [1 2 2 2 2 5 3 0 3 2 8 4]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 3 3 1 7 3 3 3 1 1 7 4]\n",
            "labels:  [1 3 3 7 4 3 3 1 1 1 7 8]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [0 1 4 9 0 3 3 1 7 1 1 1]\n",
            "labels:  [6 9 0 0 0 3 3 7 8 1 1 5]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [4 3 3 2 1 1 1 1 3 4 3 7]\n",
            "labels:  [3 3 2 2 8 1 1 8 3 4 3 7]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [0 0 3 4 0 0 0 3 2 7 4 0]\n",
            "labels:  [6 0 2 7 0 0 5 9 2 8 6 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 1 3 7 0 0 0 1 2 0 7 1]\n",
            "labels:  [1 1 2 1 0 0 0 4 2 0 4 1]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [7 3 1 1 0 3 4 3 3 2 7 1]\n",
            "labels:  [4 3 1 7 0 2 7 3 0 2 7 1]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 7 7 7 7 7 1 3 3 3 3 3]\n",
            "labels:  [5 1 7 7 0 1 1 3 1 3 1 3]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 7 3 1 3 1 8 7 0 1 3 7]\n",
            "labels:  [2 7 3 9 3 1 1 0 0 1 3 7]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [7 1 7 3 3 1 1 1 3 1 1 0]\n",
            "labels:  [4 1 4 1 3 1 7 4 2 4 3 0]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 3 7 2 1 1 4 0 1 1 0 1]\n",
            "labels:  [1 3 1 5 7 2 4 8 1 2 0 4]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 4 4 9 4 3 1 7 0 1 2 7]\n",
            "labels:  [3 7 4 2 3 2 3 7 0 1 7 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 3 7 1 1 4 1 3 1 0 3 7]\n",
            "labels:  [3 3 8 1 6 4 9 3 7 0 1 4]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [0 1 4 1 4 0 8 3 3 8 3 1]\n",
            "labels:  [0 7 0 9 1 0 8 3 3 8 1 8]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 3 4 1 7 1 0 8 4 1 3 2]\n",
            "labels:  [1 3 0 4 7 1 5 0 4 4 3 2]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 1 1 1 3 0 1 2 3 1 4 4]\n",
            "labels:  [0 2 0 0 3 7 4 2 3 1 4 4]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 3 3 3 2 0 3 2 1 7 1 0]\n",
            "labels:  [3 3 4 3 2 0 3 2 7 7 4 1]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [8 3 2 3 3 3 7 2 0 4 3 3]\n",
            "labels:  [4 3 1 3 3 3 2 2 0 1 3 3]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [3 1 4 7 1 1 1 4 7 1 4 1]\n",
            "labels:  [5 2 8 7 2 1 1 5 7 3 1 3]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [4 3 7 7 3 3 1 3 3 7 4 7]\n",
            "labels:  [0 3 1 1 3 2 1 3 7 4 7 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [4 1 2 7 7 3 2 3 4 8 3 7]\n",
            "labels:  [8 2 4 7 7 5 2 3 4 5 3 2]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 1 8 0 1 1 2 3 0 2 2 7]\n",
            "labels:  [1 1 7 8 1 1 5 5 1 2 8 8]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [8 3 0 1 1 3 1 2 4 4 1 0]\n",
            "labels:  [1 3 0 1 0 3 1 2 9 4 4 8]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [7 2 0 3 0 7 3 3 3 0 2 7]\n",
            "labels:  [7 2 9 3 0 1 8 0 1 4 2 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 0 3 3 0 7 3 3 1 3 7 1]\n",
            "labels:  [1 2 0 1 8 7 3 3 4 3 0 4]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [0 3 3 7 4 1 1 0 0 3 1 1]\n",
            "labels:  [5 3 3 7 0 1 2 0 7 3 3 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 3 4 1 3 2 0 1 4 3 4 7]\n",
            "labels:  [4 1 1 4 3 2 9 2 2 1 4 7]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [1 1 0 4 1 1 3 3 1 1 3 7]\n",
            "labels:  [2 1 8 7 4 1 3 1 1 7 5 4]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [0 0 4 3 7 3 4 7 1 3 2 4]\n",
            "labels:  [5 0 9 3 7 3 8 2 2 0 1 2]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [7 1 2 4 1 0 4 1 3 1 7 1]\n",
            "labels:  [7 4 2 8 7 7 8 5 1 1 4 7]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [0 8 0 4 7 0 2 3 7 7 1 0]\n",
            "labels:  [7 0 1 1 1 8 2 3 4 4 1 7]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [7 3 7 3 3 3 0 4 7 9 3 2]\n",
            "labels:  [7 3 7 1 0 3 1 4 7 6 3 2]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [1 1 3 9 0 7 7 4 1 3 0 3]\n",
            "labels:  [1 1 3 1 3 7 1 4 1 3 0 7]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [1 7 1 0 7 4 7 1 8 3 2 0]\n",
            "labels:  [1 7 7 0 2 4 4 1 8 2 2 0]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [7 4 0 4 0 3 2 3 4 2 7 1]\n",
            "labels:  [8 4 4 7 0 4 2 1 7 3 4 4]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [1 3 1 4 7 3 7 7 1 7 7 4]\n",
            "labels:  [2 3 4 5 7 3 7 7 1 7 7 4]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [7 2 1 7 7 7 0 3 1 8 0 9]\n",
            "labels:  [7 2 4 1 1 7 0 3 2 1 5 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [4 1 4 0 2 3 7 2 7 3 4 1]\n",
            "labels:  [4 1 4 1 1 1 7 2 7 3 4 4]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [0 4 3 1 1 4 0 4 4 0 1 3]\n",
            "labels:  [8 2 3 1 4 8 0 1 4 7 1 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [0 1 4 1 7 3 3 0 7 7 2 0]\n",
            "labels:  [0 2 4 2 2 3 1 0 7 7 2 2]\n",
            "temp eval acc:  0.5833333333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  75%|███████▌  | 3/4 [44:47<14:55, 895.62s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------------------\n",
            "pred:  [3 2 3 0 1 3 1 0 0 1 1 9]\n",
            "labels:  [2 2 1 7 4 2 1 0 5 7 2 0]\n",
            "temp eval acc:  0.25\n",
            "TOTAL EVAL LOSS:  1.8529404401779175\n",
            "Validation Accuracy: 0.48666666666666664\n",
            "F1 micro:  0.48666666666666664\n",
            "F1 macro:  0.36762034151034134\n",
            "F1 weighted:  0.46046738816738814\n",
            "Train loss: 1.301547162214915\n",
            "-----------------------\n",
            "pred:  [7 7 1 1 7 7 2 4 3 7 3 4]\n",
            "labels:  [7 4 1 1 7 2 2 8 3 4 3 4]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [4 1 3 4 2 0 0 2 2 8 4 0]\n",
            "labels:  [7 5 3 4 0 0 3 1 2 4 0 8]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [1 3 0 4 2 1 0 2 7 0 0 8]\n",
            "labels:  [1 3 0 0 1 1 1 2 0 0 5 8]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [2 2 0 0 7 1 7 4 7 7 1 0]\n",
            "labels:  [3 4 1 1 1 4 2 0 7 7 1 0]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [1 2 0 2 2 2 3 0 3 3 8 1]\n",
            "labels:  [1 1 7 1 4 2 1 9 5 3 8 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [1 1 2 2 4 1 3 0 1 0 0 4]\n",
            "labels:  [1 2 2 1 3 3 3 4 1 9 0 4]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 0 1 0 1 2 3 4 0 7 1 0]\n",
            "labels:  [2 5 1 4 1 3 1 7 0 4 1 5]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [1 7 3 0 1 2 1 2 2 0 1 7]\n",
            "labels:  [1 1 3 0 1 2 7 2 2 8 7 7]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [1 3 7 3 3 1 0 3 4 0 2 0]\n",
            "labels:  [7 3 5 2 3 2 9 3 4 0 7 2]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [4 4 4 4 3 0 0 4 1 3 7 7]\n",
            "labels:  [4 8 7 8 2 6 6 0 2 3 1 7]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [3 3 0 1 7 3 1 0 2 7 2 1]\n",
            "labels:  [3 3 4 1 0 3 1 0 7 7 6 1]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [2 7 2 2 7 1 3 2 3 4 4 0]\n",
            "labels:  [2 0 2 1 7 4 3 0 2 8 3 5]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 1 4 7 7 4 1 7 3 1 4 0]\n",
            "labels:  [3 1 4 4 5 1 1 7 3 2 4 4]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 2 3 0 2 1 1 3 0 0 0 1]\n",
            "labels:  [1 1 3 3 6 2 1 3 8 0 8 1]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [7 7 0 1 3 4 0 0 3 0 8 4]\n",
            "labels:  [7 7 5 7 8 4 3 0 3 3 0 4]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [0 3 2 3 0 1 2 3 2 7 4 3]\n",
            "labels:  [5 1 2 3 0 1 7 3 2 7 4 3]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [3 3 3 8 3 3 7 0 1 2 3 1]\n",
            "labels:  [3 3 8 8 3 5 0 0 2 2 2 1]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 0 0 7 3 0 2 7 3 4 1 0]\n",
            "labels:  [8 2 3 7 8 0 4 7 4 4 4 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [0 7 1 7 3 1 7 7 0 1 0 1]\n",
            "labels:  [3 7 2 7 3 8 7 7 0 3 7 1]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [7 9 5 0 1 2 3 4 3 3 3 7]\n",
            "labels:  [2 1 9 0 1 2 2 1 3 3 3 7]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [0 9 3 3 1 2 3 7 0 7 2 3]\n",
            "labels:  [3 9 3 3 1 8 2 2 3 0 2 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [1 7 0 4 7 8 1 3 8 7 1 4]\n",
            "labels:  [1 1 0 8 1 4 1 3 8 8 2 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 3 2 2 0 4 0 0 4 0 0 7]\n",
            "labels:  [1 3 2 1 2 7 0 0 1 0 1 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 1 3 3 2 7 7 1 0 1 7 3]\n",
            "labels:  [2 5 3 0 2 7 7 8 0 0 7 3]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 3 4 0 7 1 7 0 7 2 1 4]\n",
            "labels:  [8 3 1 5 0 0 7 7 7 2 1 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [7 9 2 7 1 1 2 3 0 2 2 2]\n",
            "labels:  [0 9 7 1 4 1 2 3 5 7 1 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [4 2 3 4 2 0 4 4 1 3 8 1]\n",
            "labels:  [8 2 3 4 2 1 5 4 1 3 0 1]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [0 0 2 3 7 4 4 3 2 9 4 0]\n",
            "labels:  [0 8 2 3 8 4 4 3 4 9 4 0]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [9 3 1 1 4 4 1 3 1 1 7 2]\n",
            "labels:  [3 3 8 4 3 1 1 1 4 3 1 1]\n",
            "temp eval acc:  0.16666666666666666\n",
            "-----------------------\n",
            "pred:  [7 2 4 7 1 4 1 2 2 2 1 7]\n",
            "labels:  [4 2 1 7 1 1 1 2 2 2 1 0]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [0 7 7 1 3 0 3 3 0 3 2 0]\n",
            "labels:  [0 4 7 1 3 7 8 3 7 3 2 7]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 0 2 1 7 2 7 0 4 1 3 7]\n",
            "labels:  [4 0 2 5 7 2 9 0 4 1 7 4]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [0 8 3 2 1 1 4 2 0 4 4 7]\n",
            "labels:  [9 7 0 2 1 1 0 2 9 8 7 3]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [1 3 7 0 0 2 7 8 3 7 2 7]\n",
            "labels:  [1 3 7 1 7 2 7 8 3 4 2 1]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [0 0 0 3 2 1 2 1 3 1 1 3]\n",
            "labels:  [3 0 0 3 0 1 2 1 3 7 4 3]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [0 4 0 1 7 7 0 7 1 3 1 2]\n",
            "labels:  [0 1 1 8 7 7 4 1 2 6 1 7]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [2 2 1 0 3 3 1 0 2 3 4 1]\n",
            "labels:  [5 2 2 8 6 1 1 0 1 3 4 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 7 1 7 1 1 4 3 3 4 1 2]\n",
            "labels:  [3 0 1 1 1 2 8 3 2 3 8 2]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [2 8 1 0 8 1 3 0 0 7 0 1]\n",
            "labels:  [2 0 1 1 7 1 8 2 0 7 0 0]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 3 2 1 3 1 7 3 4 0 8 0]\n",
            "labels:  [3 3 2 7 3 1 7 1 9 7 8 2]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [2 0 1 4 1 3 7 4 1 0 0 3]\n",
            "labels:  [2 0 2 1 7 3 7 7 7 0 8 3]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 7 3 1 2 0 7 1 1 1 1 4]\n",
            "labels:  [3 4 3 1 2 6 7 0 8 8 1 8]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 1 1 4 0 4 0 3 3 4 1 0]\n",
            "labels:  [1 1 2 9 7 4 7 3 3 2 3 0]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [8 8 0 4 7 2 2 0 0 1 4 0]\n",
            "labels:  [8 1 4 4 1 2 1 0 6 1 7 9]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [7 7 3 2 1 0 7 1 0 0 7 0]\n",
            "labels:  [7 7 3 8 8 0 4 1 7 0 1 5]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 7 7 7 3 3 4 7 2 0 1 2]\n",
            "labels:  [3 4 8 7 1 3 4 0 7 0 2 2]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [0 7 0 1 7 3 7 0 1 0 3 0]\n",
            "labels:  [8 8 4 4 7 3 4 2 1 2 3 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 7 7 8 1 7 1 1 7 1 3 1]\n",
            "labels:  [6 2 1 8 1 7 7 8 4 1 1 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [4 1 1 0 2 7 0 2 0 3 4 4]\n",
            "labels:  [5 4 4 4 2 8 0 2 0 3 4 8]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [7 8 0 4 2 0 7 2 3 8 4 2]\n",
            "labels:  [0 0 7 4 7 5 9 2 3 0 1 1]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [4 4 8 4 0 7 1 4 1 2 1 1]\n",
            "labels:  [4 5 7 4 1 7 4 7 1 2 1 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [7 2 1 2 2 7 1 0 3 2 2 0]\n",
            "labels:  [1 5 1 2 8 7 2 8 3 7 4 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [2 3 4 4 1 1 8 8 0 2 8 7]\n",
            "labels:  [2 1 4 4 2 1 1 5 9 1 5 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [8 3 0 2 7 2 4 4 0 2 2 7]\n",
            "labels:  [8 3 0 1 7 2 4 7 0 8 1 7]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [2 3 4 7 2 7 0 7 0 3 1 0]\n",
            "labels:  [4 8 3 1 1 0 2 7 2 3 2 1]\n",
            "temp eval acc:  0.16666666666666666\n",
            "-----------------------\n",
            "pred:  [0 1 3 7 1 3 1 2 0 3 2 7]\n",
            "labels:  [0 1 1 7 1 3 1 2 5 3 2 7]\n",
            "temp eval acc:  0.8333333333333334\n",
            "-----------------------\n",
            "pred:  [0 4 3 0 0 8 2 0 3 4 3 2]\n",
            "labels:  [0 4 3 0 4 4 8 5 3 4 3 2]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [0 3 1 0 2 7 7 0 0 3 4 7]\n",
            "labels:  [0 3 8 2 1 7 0 0 8 3 4 7]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 7 7 1 3 2 1 4 0 3 4 0]\n",
            "labels:  [4 1 4 1 1 1 7 4 2 3 4 4]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 3 0 8 0 9 0 3 1 7 0 3]\n",
            "labels:  [3 3 1 4 1 9 8 3 4 0 0 3]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 0 0 2 3 4 3 2 0 1 2 3]\n",
            "labels:  [3 0 8 7 2 0 3 2 7 1 2 3]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 7 2 0 2 3 7 4 3 0 3 1]\n",
            "labels:  [3 3 1 0 4 3 0 1 3 0 2 1]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 1 0 7 5 3 0 2 2 9 2 8]\n",
            "labels:  [3 1 0 7 5 3 0 2 2 2 8 8]\n",
            "temp eval acc:  0.8333333333333334\n",
            "-----------------------\n",
            "pred:  [4 0 3 4 2 1 8 4 0 0 3 1]\n",
            "labels:  [5 0 3 1 1 1 5 4 0 5 3 0]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [4 1 1 0 3 0 9 1 1 4 0 0]\n",
            "labels:  [4 2 1 8 3 2 9 4 1 3 2 7]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [2 7 3 3 8 0 3 0 7 0 3 0]\n",
            "labels:  [2 4 3 3 7 8 1 7 7 4 3 7]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 4 0 1 7 1 0 7 7 3 4 3]\n",
            "labels:  [3 4 3 2 8 7 7 7 0 3 0 2]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [0 2 8 7 0 7 7 0 4 8 0 0]\n",
            "labels:  [0 7 0 3 2 1 2 0 4 5 5 2]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [7 2 7 0 8 1 3 7 1 3 9 7]\n",
            "labels:  [1 3 2 0 9 1 8 8 4 2 9 0]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [1 0 2 4 3 8 7 3 0 7 3 7]\n",
            "labels:  [1 3 3 5 2 1 7 3 2 7 1 1]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [0 0 0 7 1 2 0 7 3 0 2 1]\n",
            "labels:  [3 0 6 0 1 2 2 1 3 0 1 1]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 1 1 8 7 1 0 1 1 1 0 1]\n",
            "labels:  [1 4 1 8 7 0 4 1 1 0 4 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [2 8 7 0 7 7 3 8 7 3 2 2]\n",
            "labels:  [6 7 4 0 7 7 2 4 9 3 2 2]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [8 3 7 1 1 0 1 7 4 0 1 0]\n",
            "labels:  [2 3 7 1 2 0 1 5 4 0 0 2]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 2 2 0 7 1 3 1 7 1 0 2]\n",
            "labels:  [1 2 4 2 5 3 3 3 7 1 0 0]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 2 7 0 0 0 1 4 8 0 3 0]\n",
            "labels:  [3 2 0 8 0 9 1 4 0 9 1 0]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [0 1 0 0 8 3 3 8 0 3 7 0]\n",
            "labels:  [0 1 0 1 4 1 3 5 1 3 1 7]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [2 3 3 4 3 1 7 8 3 8 1 1]\n",
            "labels:  [2 3 3 0 3 1 7 7 3 4 1 4]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [9 0 1 7 2 4 4 1 3 0 7 4]\n",
            "labels:  [0 0 4 2 2 4 6 4 3 0 7 4]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [7 7 2 0 0 8 1 0 7 3 1 0]\n",
            "labels:  [2 7 2 0 0 8 6 0 4 0 1 8]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [0 3 3 1 4 1 4 4 7 9 0 7]\n",
            "labels:  [0 1 3 2 1 1 7 4 7 8 1 7]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [3 7 7 1 7 7 2 2 3 3 4 7]\n",
            "labels:  [3 2 1 3 7 2 2 2 3 1 2 8]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [3 7 1 3 2 3 3 3 7 7 7 8]\n",
            "labels:  [2 7 1 3 2 3 1 3 7 1 9 3]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [7 3 1 1 0 1 7 7 7 3 8 3]\n",
            "labels:  [8 3 1 1 5 1 7 7 8 3 2 3]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [1 8 2 7 0 8 1 7 0 1 8 4]\n",
            "labels:  [1 2 2 2 2 5 3 0 3 2 8 4]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 3 0 1 7 3 3 3 1 1 7 4]\n",
            "labels:  [1 3 3 7 4 3 3 1 1 1 7 8]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [0 0 4 1 0 3 3 1 8 1 1 1]\n",
            "labels:  [6 9 0 0 0 3 3 7 8 1 1 5]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [4 3 2 2 1 1 7 1 3 4 3 7]\n",
            "labels:  [3 3 2 2 8 1 1 8 3 4 3 7]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [0 0 3 8 0 8 0 3 2 7 7 0]\n",
            "labels:  [6 0 2 7 0 0 5 9 2 8 6 0]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 1 1 7 0 0 0 1 2 0 7 7]\n",
            "labels:  [1 1 2 1 0 0 0 4 2 0 4 1]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [7 3 1 2 0 3 7 3 3 2 7 2]\n",
            "labels:  [4 3 1 7 0 2 7 3 0 2 7 1]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 7 7 7 7 7 1 3 3 3 3 1]\n",
            "labels:  [5 1 7 7 0 1 1 3 1 3 1 3]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [1 7 3 0 3 1 8 7 0 1 3 7]\n",
            "labels:  [2 7 3 9 3 1 1 0 0 1 3 7]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [7 1 7 3 3 1 1 3 3 1 1 0]\n",
            "labels:  [4 1 4 1 3 1 7 4 2 4 3 0]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 3 8 2 1 1 4 0 1 3 0 2]\n",
            "labels:  [1 3 1 5 7 2 4 8 1 2 0 4]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [3 7 4 0 0 2 1 7 0 1 2 0]\n",
            "labels:  [3 7 4 2 3 2 3 7 0 1 7 0]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [3 3 8 2 1 4 1 3 1 0 3 7]\n",
            "labels:  [3 3 8 1 6 4 9 3 7 0 1 4]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [0 1 0 4 8 0 8 3 3 8 3 1]\n",
            "labels:  [0 7 0 9 1 0 8 3 3 8 1 8]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [3 3 8 1 7 1 0 8 4 1 3 2]\n",
            "labels:  [1 3 0 4 7 1 5 0 4 4 3 2]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [1 2 1 0 3 0 1 2 3 1 4 4]\n",
            "labels:  [0 2 0 0 3 7 4 2 3 1 4 4]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [3 3 3 3 2 0 3 2 1 7 2 0]\n",
            "labels:  [3 3 4 3 2 0 3 2 7 7 4 1]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [8 3 2 3 3 0 2 2 0 7 3 3]\n",
            "labels:  [4 3 1 3 3 3 2 2 0 1 3 3]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [1 1 8 7 1 2 1 8 7 1 0 0]\n",
            "labels:  [5 2 8 7 2 1 1 5 7 3 1 3]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [0 1 7 7 3 2 1 3 3 7 4 7]\n",
            "labels:  [0 3 1 1 3 2 1 3 7 4 7 1]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [4 2 2 7 7 3 2 3 4 8 3 7]\n",
            "labels:  [8 2 4 7 7 5 2 3 4 5 3 2]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 1 8 0 1 1 2 3 0 2 2 7]\n",
            "labels:  [1 1 7 8 1 1 5 5 1 2 8 8]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [8 0 0 1 2 3 1 2 0 4 1 2]\n",
            "labels:  [1 3 0 1 0 3 1 2 9 4 4 8]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [7 2 0 3 0 7 1 0 3 0 2 7]\n",
            "labels:  [7 2 9 3 0 1 8 0 1 4 2 7]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [2 0 3 3 0 7 3 3 1 3 7 1]\n",
            "labels:  [1 2 0 1 8 7 3 3 4 3 0 4]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [0 1 3 7 4 1 1 0 0 3 1 1]\n",
            "labels:  [5 3 3 7 0 1 2 0 7 3 3 7]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [1 3 4 7 3 2 0 0 2 3 8 7]\n",
            "labels:  [4 1 1 4 3 2 9 2 2 1 4 7]\n",
            "temp eval acc:  0.3333333333333333\n",
            "-----------------------\n",
            "pred:  [2 1 0 3 1 7 0 3 1 1 3 7]\n",
            "labels:  [2 1 8 7 4 1 3 1 1 7 5 4]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [0 0 7 1 7 3 4 7 2 0 2 3]\n",
            "labels:  [5 0 9 3 7 3 8 2 2 0 1 2]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [7 1 2 4 7 0 4 2 2 1 7 7]\n",
            "labels:  [7 4 2 8 7 7 8 5 1 1 4 7]\n",
            "temp eval acc:  0.4166666666666667\n",
            "-----------------------\n",
            "pred:  [0 8 8 4 7 0 2 3 7 7 1 0]\n",
            "labels:  [7 0 1 1 1 8 2 3 4 4 1 7]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [7 3 7 2 3 3 7 8 7 9 3 2]\n",
            "labels:  [7 3 7 1 0 3 1 4 7 6 3 2]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [1 1 3 9 0 7 7 4 1 3 0 0]\n",
            "labels:  [1 1 3 1 3 7 1 4 1 3 0 7]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [1 7 1 0 7 4 7 1 8 3 2 0]\n",
            "labels:  [1 7 7 0 2 4 4 1 8 2 2 0]\n",
            "temp eval acc:  0.6666666666666666\n",
            "-----------------------\n",
            "pred:  [7 4 0 4 0 3 2 3 4 2 7 1]\n",
            "labels:  [8 4 4 7 0 4 2 1 7 3 4 4]\n",
            "temp eval acc:  0.25\n",
            "-----------------------\n",
            "pred:  [1 3 4 7 7 3 7 7 2 7 7 4]\n",
            "labels:  [2 3 4 5 7 3 7 7 1 7 7 4]\n",
            "temp eval acc:  0.75\n",
            "-----------------------\n",
            "pred:  [7 2 7 7 7 7 0 3 2 8 0 0]\n",
            "labels:  [7 2 4 1 1 7 0 3 2 1 5 0]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [4 1 0 8 2 2 7 2 7 3 4 1]\n",
            "labels:  [4 1 4 1 1 1 7 2 7 3 4 4]\n",
            "temp eval acc:  0.5833333333333334\n",
            "-----------------------\n",
            "pred:  [0 0 3 1 0 4 0 4 4 0 1 0]\n",
            "labels:  [8 2 3 1 4 8 0 1 4 7 1 0]\n",
            "temp eval acc:  0.5\n",
            "-----------------------\n",
            "pred:  [0 2 7 4 7 3 1 9 7 7 2 0]\n",
            "labels:  [0 2 4 2 2 3 1 0 7 7 2 2]\n",
            "temp eval acc:  0.5833333333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 4/4 [59:43<00:00, 895.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------------------\n",
            "pred:  [3 2 3 0 1 0 1 0 7 1 2 9]\n",
            "labels:  [2 2 1 7 4 2 1 0 5 7 2 0]\n",
            "temp eval acc:  0.3333333333333333\n",
            "TOTAL EVAL LOSS:  1.484209418296814\n",
            "Validation Accuracy: 0.49133333333333334\n",
            "F1 micro:  0.49133333333333334\n",
            "F1 macro:  0.37685420531849106\n",
            "F1 weighted:  0.4710328523328523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKlKHCREQgbO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "222db634-5127-42ab-b4ff-943e5b2178d7"
      },
      "source": [
        "errors"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': 313,\n",
              " '1': 687,\n",
              " '2': 406,\n",
              " '3': 175,\n",
              " '4': 419,\n",
              " '5': 223,\n",
              " '6': 72,\n",
              " '7': 338,\n",
              " '8': 376,\n",
              " '9': 114}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7KluvTxQiWV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "dded7080-9e80-4eee-86aa-e1b4174744bd"
      },
      "source": [
        "total"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': 748,\n",
              " '1': 1184,\n",
              " '2': 824,\n",
              " '3': 948,\n",
              " '4': 676,\n",
              " '5': 224,\n",
              " '6': 72,\n",
              " '7': 788,\n",
              " '8': 408,\n",
              " '9': 128}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tgcVV5-6Brb",
        "colab_type": "code",
        "outputId": "0a84a997-1c3b-4fd7-8b78-700be24b7fa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "# If we save using the predefined names, we can load using `from_pretrained`\n",
        "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
        "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
        "\n",
        "torch.save(model_to_save.state_dict(), output_model_file)\n",
        "model_to_save.config.to_json_file(output_config_file)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "# model_to_save.save_pretrained(output_dir)\n",
        "# tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/vocab.txt',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFjkBBXE4By2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in test_sent:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 512,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(test_labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', test_sent[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vym7fTpq42Jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_inputs, _, test_labels, _ = train_test_split(input_ids, labels, \n",
        "                                                          random_state=2018, test_size=0.1)\n",
        "test_masks, _, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "\n",
        "test_inputs = torch.tensor(test_inputs)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oBd84zu0qO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "#initialize vars\n",
        "f1_micr_test = 0\n",
        "f1_macr_test = 0\n",
        "f1_weight_test = 0\n",
        "nb_eval_steps_test = 0\n",
        "\n",
        "# Predict \n",
        "for batch in test_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  # predictions.append(logits)\n",
        "  # true_labels.append(label_ids)\n",
        "\n",
        "  tmp_f1_micr, tmp_f1_macr, tmp_f1_weight = f1score(label_ids, logits)\n",
        "\n",
        "  #calculate f1 scores\n",
        "  f1_micr_test += tmp_f1_micr\n",
        "  f1_macr_test += tmp_f1_macr\n",
        "  f1_weight_test += tmp_f1_weight\n",
        "  nb_eval_steps_test += 1\n",
        "\n",
        "print('F1 micro test: ', f1_micr_test/nb_eval_steps_test)\n",
        "print('F1 macro test: ', f1_macr_test/nb_eval_steps_test)\n",
        "print('F1 weight test: ', f1_weight_test/nb_eval_steps_test)\n",
        "print('    DONE.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD8hBq0_-Kav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}